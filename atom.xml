<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jiawen and her funny friends</title>
  
  
  <link href="http://yoursite.com/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2023-03-24T11:39:35.026Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jiawen Zhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deep Generative Models</title>
    <link href="http://yoursite.com/2023/03/24/Deep-Generative-Models/"/>
    <id>http://yoursite.com/2023/03/24/Deep-Generative-Models/</id>
    <published>2023-03-24T09:01:34.000Z</published>
    <updated>2023-03-24T11:39:35.026Z</updated>
    
    <content type="html"><![CDATA[<p>Understand and create complex, unstructured data.</p><span id="more"></span><h1 id="autoregressive-models">Autoregressive Models</h1><ul><li>Chain rule based factorization is fully general</li><li>Compact representation via conditional independence and/or neuralparameterizations</li></ul><h2 id="parameterize-a-model-family">Parameterize a model family</h2><p>Without loss of generality, we can use chain rule forfactorization</p><p><span class="math display">\[p(x_1,...,x_n) = p(x_1)p(x_2|x_1) \cdotsp(x_n|x_1,...,x_{n-1})\]</span></p><h3 id="fully-visible-sigmoid-belief-network-fvsbn">Fully VisibleSigmoid Belief Network (FVSBN)</h3><p>The conditional variables <span class="math inline">\(X_i|X_1,···,X_{i−1}\)</span> are Bernoulli with parameters</p><p><span class="math display">\[\hat{x}_i = p(X_i=1| x_1, \cdots,x_{i-1}; \bf{\alpha}^i) = p(X_i|x_{&lt;i};\bf{\alpha}^i) = \sigma(\alpha^i_0 + \sum^{i-1}_{j=1} \alpha^i_j x_j)\]</span></p><h3 id="neural-autoregressive-density-estimation-nade">NeuralAutoregressive Density Estimation (NADE)</h3><p>Improvement: Use one layer neural network instead of logisticregression.</p><p><span class="math display">\[\hat{x}_i = p(x_i | x_1, \cdots,x_{i-1}; A_i,c_i, \bf{\alpha}_i, b_i) = \sigma (\bf{\alpha}_i h_i +b_i)\]</span></p><p><strong>Autoregressive models vs. autoencoders:</strong> - A vanillaautoencoder is not a generative model: it does not define a distributionover x we can sample from to generate new data points. - Forautoregressive autoencoders: We need to make sure it corresponds to avalid Bayesian Network (DAG structure), i.e., we need an<em>ordering</em>. - <span class="math inline">\(\hat{x}_1\)</span>cannot depend on any input <span class="math inline">\(x\)</span>. Thenat generation time we don’t need any input to get started. - <spanclass="math inline">\(\hat{x}_2\)</span> can only depend on <spanclass="math inline">\(x_1\)</span></p><h3 id="masked-autoencoder-for-distribution-estimation-made">MaskedAutoencoder for Distribution Estimation (MADE)</h3><p><a href="https://imgse.com/i/pS53XRJ"><imgsrc="https://s1.ax1x.com/2023/02/12/pS53XRJ.md.png"alt="MADE.png" /></a></p><p>Use masks to disallow certain paths (Germain et al., 2015). Supposeordering is <span class="math inline">\(x_2, x_3, x_1\)</span>.</p><ol type="1"><li>The unit producing the parameters for <spanclass="math inline">\(p(x_2)\)</span> is not allowed to depend on anyinput. Unit for <span class="math inline">\(p(x_3|x_2)\)</span> only on<span class="math inline">\(x_2\)</span>. And so on...</li><li>For each unit in a hidden layer, pick a random integer <spanclass="math inline">\(i\)</span> in <span class="math inline">\([1, n −1]\)</span>. That unit is allowed to depend only on the first <spanclass="math inline">\(i\)</span> inputs (according to the chosenordering).</li><li>Add mask to preserve this invariant: connect to all units inprevious layer with smaller or equal assigned number (strictly &lt; infinal layer)</li></ol><h2 id="pros-cons">Pros &amp; Cons</h2><h3 id="pros">Pros</h3><ul><li>Easy to evaluate likelihoods</li><li>Easy to train</li></ul><h3 id="cons">Cons</h3><ul><li>Requires an ordering</li><li>Generation is sequential</li><li>Cannot learn features in an unsupervised way</li></ul><h2 id="search-for-model-parameters">Search for model parameters</h2><h3 id="kullback-leibler-divergence-kl-divergence">Kullback-Leiblerdivergence (KL-divergence)</h3><p><span class="math display">\[D(p||q) = \sum_x p(x) \log\frac{p(x)}{q(x)}\]</span></p><ul><li><span class="math inline">\(D(p||q) \geq 0\)</span> for all <spanclass="math inline">\(p,q\)</span> with equality if and only if <spanclass="math inline">\(p = q\)</span>.</li><li>Asymmetric, <span class="math inline">\(D(p||q) \neqD(q||p)\)</span>.</li><li>If data comes from <span class="math inline">\(p\)</span>, but use ascheme optimized for <span class="math inline">\(q\)</span>, thedivergence <span class="math inline">\(DKL(p||q)\)</span> is the numberof extra bits need on average.</li></ul><h3 id="expected-log-likelihood">Expected log-likelihood</h3><p><span class="math display">\[D(P_{data}||P_{\theta)} = E_{x \backsimP_{data}} [ \log \frac{P_{data}(x)}{P_{\theta}(x)} ] \\= E_{x \backsim P_{data}} [ \log P_{data}(x)] - E_{x \backsimP_{data}}  [P_{\theta}(x) ]\]</span></p><p>The first term does not depend on <spanclass="math inline">\(P_{\theta\)</span>, then minimizing KL divergenceis equivalent to maximizing the expected log-likelihood</p><p><span class="math display">\[\arg \min_{P_{\theta}} D(P_{data} ||P_{\theta}) = \arg \min_{P_{\theta}} - E_{x \backsim P_{data}} [\logP_{\theta} (x)] = \arg \max_{P_{\theta}} E_{x \backsim P_{data}} [\logP_{\theta}]\]</span></p><p><strong>Empirical log-likelihood:</strong></p><p><span class="math display">\[E_{\mathcal{D}} [\log P_{\theta} (x)] =\frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \log P_{\theta}(x)\]</span></p><p><strong>Maximum likelihood learning:</strong></p><p><span class="math display">\[\max_{P_{\theta}}\frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \log P_{\theta}(x)\]</span></p><h3 id="main-idea-in-monte-carlo-estimation">Main idea in Monte CarloEstimation</h3><ol type="1"><li>Express the quantity of interest as the expected value of a randomvariable.</li></ol><p><span class="math display">\[E_{x \backsim P} [g(x)] = \sum_x g(x)P(x)\]</span></p><ol start="2" type="1"><li>Generate T samples <span class="math inline">\(x_1, . . . ,x_T\)</span> from the distribution <spanclass="math inline">\(P\)</span> with respect to which the expectationwas taken.</li><li>Estimate the expected value from the samples using:</li></ol><p><span class="math display">\[\hat{g} (x^1,...,x^{T}) \triangleq\frac{1}{T} \sum^{T}_{t=1} g(x^t)\]</span> where <spanclass="math inline">\(x^1,...,x^T\)</span> are independent samples from<span class="math inline">\(P\)</span>, <spanclass="math inline">\(\hat{g}\)</span> is a random variable.</p><h4 id="properties-of-the-monte-carlo-estimate">Properties of the MonteCarlo Estimate</h4><ul><li><p>Unbiased <span class="math display">\[E_P [\hat{g}] = E_P[g(x)]\]</span></p></li><li><p>Convergence <span class="math display">\[\hat{g} = \frac{1}{T}\sum^T_{t=1} g(x^t) \rightarrow E_P [g(x)] \quad for \quad T \rightarrow\infty\]</span></p></li><li><p>Variance <span class="math display">\[V_P[\hat{g}] = V_P[\frac{1}{T} \sum^T_{t=1}g(x^t)] = \frac{V_P [g(x)]}{T}\]</span>variance of the estimator can be reduced by increasing the number ofsamples</p></li></ul><h3 id="empirical-risk-and-overfitting">Empirical Risk andOverfitting</h3><ul><li>Empirical risk minimization can easily overfit the data</li><li>Generalization: Model should generalize well to these “never-seen”samples.</li></ul><p><strong>Bias-Variance trade off:</strong></p><ul><li>If the hypothesis space is very limited, it might not be able torepresent <span class="math inline">\(P_{data}\)</span>, even withunlimited data (<strong>bias</strong>)</li><li>If we select a highly expressive hypothesis class, we mightrepresent better the data. (<strong>variance</strong>)<ul><li>When we have small amount of data, multiple models can fit well, oreven better than the true model. Moreover, small perturbations on <spanclass="math inline">\(\mathcal{D}\)</span> will result in very differentestimates</li></ul></li></ul><h4 id="avoid-overfitting">Avoid overfitting</h4><ul><li>Hard constraints, e.g. by selecting a less expressive model family<ul><li>Smaller neural networks with less parameters</li><li>Weight sharing</li></ul></li><li>Soft preference for “simpler” models: <strong>OccamRazor</strong>.</li><li>Augment the objective function with regularization</li><li>Evaluate generalization performance on a held-out validationset</li></ul><h1 id="latent-variable-models">Latent Variable Models</h1><h2 id="mixture-of-gaussians-a-shallow-latent-variable-model">Mixture ofGaussians: a Shallow Latent Variable Model</h2><p>Mixture of Gaussians. Bayes net: <span class="math inline">\(z\rightarrow x\)</span>. - <span class="math inline">\(z \backsim\textrm{Categorical}(1,...,K)\)</span> - <spanclass="math inline">\(p(x|z=k) = \mathcal{N}(\mu_k,\Sigma_k)\)</span></p><p>Generative process: 1. Pick a mixture component <spanclass="math inline">\(k\)</span> by sampling <spanclass="math inline">\(z\)</span> 2. Generate a data point by samplingfrom that Gaussian</p><h2 id="variational-autoencoder">Variational Autoencoder</h2><p>A mixture of an infinite number of Gaussians: 1. <spanclass="math inline">\(z \backsim \mathcal{N}(0,I)\)</span> 2. <spanclass="math inline">\(p(x|z) = \mathcal{N}(\mu_{\theta}(z),\Sigma_{\theta} (z))\)</span> where <spanclass="math inline">\(\mu_{\theta}, \Sigma_{\theta}\)</span> are neuralnetworks - <span class="math inline">\(\mu_{\theta} (z) - \sigma(Az + c)= (\sigma (a_1 z + c_1), \sigma (a_2 z + c_2)) = (\mu_1 (z), \mu_2(z))\)</span> - $_{} (z) = (((B z + d))) = $ - <spanclass="math inline">\(\theta = (A,B,c,d)\)</span> 3. Even though <spanclass="math inline">\(p(x | z)\)</span> is simple, the marginal <spanclass="math inline">\(p(x)\)</span> is very complex/flexible</p><p>隐变量z是一个多维高斯分布，<spanclass="math inline">\(p(x|z)\)</span>是一个神经网络，也是一个高斯分布。整个隐变量模型可以视为无数个高斯分布的组合，<spanclass="math inline">\(x\)</span>在不同的地方对应不同的高斯分布： <spanclass="math display">\[p(x) = \int p(x|z)p(z)dz\]</span></p><p>MLE目标是 <span class="math display">\[\theta \leftarrow \arg\max_{\theta} \frac{1}{N} \sum_i \log (\int p_{\theta}(x_i|z)p(z)dz)\]</span>上述积分不可计算，我们先把积分写成期望形式，假设<spanclass="math inline">\(z \backsim p(z|x_i)\)</span> <spanclass="math display">\[\theta \leftarrow \arg \max_{\theta} \frac{1}{N}\sum_i E_{z \backsim p(z|x_i)} [ \log p_{\theta} (x_i|z)]\]</span>我们不需要对所有这都计算<span class="math inline">\(p_{\theta}(x_i|z)\)</span>,只有少部分z会导致<spanclass="math inline">\(x_i\)</span>的产生,这少部分z的分布为<spanclass="math inline">\(p(z|x_i)\)</span>，可记为<spanclass="math inline">\(q_i(z)\)</span>,其分布可能非常复杂（后面令其神经网络化）。<span class="math inline">\(q_i (z)\)</span>无法被用于精确计算<spanclass="math inline">\(\log p_{\theta}(x_i)\)</span>，但可求其下界，i.e.,变分推断。</p><h3 id="marginal-likelihood-边缘似然">Marginal Likelihood 边缘似然</h3><pre><code>概率：已知一些参数的情况下，预测接下来观测所得到的结果。似然性：已知的某些观测所得到的结果时，对有关事物的性质的参数进行估计。</code></pre><p>在统计学中，边缘似然函数（marginal likelihoodfunction），或者积分似然（integratedlikelihood），是一个某些参数变量边缘化的似然函数（likelihoodfunction）。在贝叶斯统计范畴，它可以被称作为证据或者模型证据的。</p><h3 id="evidence-lower-bound">Evidence Lower Bound</h3><p>Log-Likelihood function for Partially Observed Data is hard tocompute: <span class="math display">\[\log (\sum_{z \in \mathcal{Z}}p_{\theta} (\bf{x}, \bf{z})) = \log (\sum_{z \in \mathcal{Z}}\frac{q(z)}{q(z)} p_{\theta (\bf{x}, \bf{z})}) = \log (E_{z \backsimq(z)} [ \frac{p_{\theta} (\bf{x}, \bf{z})}{q(z)} ])\]</span></p><ul><li>log() is a concave function. <span class="math inline">\(\log(px +(1 − p)x^′) ≥ p \log(x) + (1 − p) log(x^′)\)</span>.</li><li>Idea: use Jensen Inequality (for concave functions):<spanclass="math inline">\(\log E[y] \geq E[\log y]\)</span>: <spanclass="math display">\[\log (E_{z \backsim q(z)} [f(z)]) = \log (\sum_zq(z)f(z)) ≥ \sum_z q(z) \log f(z)\]</span></li></ul><p>Choosing <spanclass="math inline">\(f(z)=\frac{p_{\theta}(\bf{x},\bf{z})}{q(z)}\)</span><span class="math display">\[\log (E_{z \backsim q(z)}[\frac{p_{\theta}(\bf{x}, \bf{z})}{q(z)}]) ≥ E_{z \backsim q(z)} [\log(\frac{p_{\theta (\bf{x}, \bf{z})}}{q(z)})]\]</span> Called EvidenceLower Bound (<strong>ELBO</strong>).</p><h3 id="variational-inference">Variational inference</h3><p>Suppose <span class="math inline">\(q(z)\)</span> is any probabilitydistribution over the hidden variables, evidence lower bound (ELBO)holds for any <span class="math inline">\(q\)</span> <spanclass="math display">\[\begin{aligned}\log p(x;\theta) &amp;\geq \sum_z q(z) \log(\frac{p_{\theta}(\bf{x},\bf{z})}{q(z)}) \\ &amp;= \sum_xq(z) \logp_{\theta} (\bf{x},\bf{z}) - \sum_z q(z) \log q(z) \\ &amp;=\sum_z q(z)\log p_{\theta} (\bf{x},\bf{z}) + H(q)\end{aligned}\]</span></p><p>Equality holds if <span class="math inline">\(q =p(z|x;\theta)\)</span> <span class="math display">\[\log p(x;\theta) =\sum_z q(z) \log p(z,x;\theta) + H(q)\]</span></p><p>In general, <span class="math inline">\(\log p(x;theta) =\textrm{ELBO} + D_{KL} (q(z) || p(z|x;\theta))\)</span>, the closer<span class="math inline">\(q(z)\)</span> is to the posterior <spanclass="math inline">\(p(z|x;\theta)\)</span> the closer the ELBO is tothe ture log-likelihood.</p><p><span class="math display">\[\begin{aligned}\log p(x;\theta) &amp;\geq \sum_z q(z;\phi) \log p(z,x;\theta) +H(q(z;\phi)) \\&amp;= \mathcal{L}(x;\theta,\phi) + D_{KL} (q(z;\phi)|| p(z|x;\theta))\end{aligned}\]</span></p><p>or <span class="math display">\[\begin{aligned}\mathcal{L} = E_{z \backsim q_{\phi}(z|x)} [\log p_{\theta} (x|z) + \logp(z)] + H(q_{\phi}(z|x))\end{aligned}\]</span></p><p><spanclass="math inline">\(\mathcal{L}(x;\theta,\phi)\)</span>为ELBO，通过最大化ELBO将下界推高。由于KL散度永远大于0，<span class="math inline">\(\logp(x_i)\)</span>的值不随<span class="math inline">\(q_i(z)\)</span>的变化而变化。当使<span class="math inline">\(q_i(z)\)</span>逼近<span class="math inline">\(p(z|x_i)\)</span>时，<spanclass="math inline">\(\mathcal{L}(x;\theta,\phi)\)</span>能取最大值，因此我们需要最小化<spanclass="math inline">\(D_{KL} (q(z;\phi)|| p(z|x;\theta))\)</span></p><h1 id="normalizing-flow-models">Normalizing Flow Models</h1><p><strong>Key idea behind flow models:</strong> Map simpledistributions (easy to sample and evaluate densities) to complexdistributions through an invertible transformation.</p><ul><li>Typically consider parameterized densities:<ul><li>Gaussian: <span class="math inline">\(X \backsim \mathcal{N}(\mu,\sigma)\)</span> if <span class="math inline">\(p_x(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-(x-\mu)^2/2\sigma^2}\)</span></li><li>Uniform: <span class="math inline">\(X \backsim\mathcal{U}(a,b)\)</span> if <span class="math inline">\(p_x(x)=\frac{1}{b-a} 1[a \leq x \leq b]\)</span></li></ul></li><li>If <span class="math inline">\(X\)</span> is a continuous randomvector, we can usually represent it using its <strong>joint probabilitydensity function</strong>:<ul><li>Gaussian: if <span class="math inline">\(p_x (x) =\frac{1}{\sqrt{(2\pi)^n} |\Sigma|} \exp (-\frac{1}{2} (x - \mu )^T\Sigma^{-1} (x - \mu))\)</span></li></ul></li></ul><h2 id="change-of-variables-formula">Change of Variables formula</h2><p>Change of variables (1D case): If <spanclass="math inline">\(X=f(Z)\)</span> and <spanclass="math inline">\(f(·)\)</span> is monotone with inverse <spanclass="math inline">\(Z=f^{−1}(X)=h(X)\)</span>, then: <spanclass="math display">\[p_X (x) = p_Z (h(x)) |h^{&#39;} (x)|\]</span>Note that the ”shape” of <span class="math inline">\(p_X(x)\)</span> isdifferent (more complex) from that of the prior <spanclass="math inline">\(p_Z(z)\)</span>. Letting <spanclass="math inline">\(z=h(x)=f^{−1}(x)\)</span> we can also write <spanclass="math display">\[p_X (x) = p_Z (z) \frac{1}{f^{&#39;}(z)}\]</span></p><h2 id="generalized-change-of-variables">Generalized change ofvariables</h2><ul><li>For linear transformations specified via <spanclass="math inline">\(A\)</span>, change in volume is given by thedeterminant of <span class="math inline">\(A\)</span></li><li>For non-linear transformations <spanclass="math inline">\(f(·)\)</span>, the linearized change in volume isgiven by the determinant of the Jacobian of <spanclass="math inline">\(f(·)\)</span></li><li><strong>Change of variables (General case):</strong> The mappingbetween <span class="math inline">\(Z\)</span> and <spanclass="math inline">\(X\)</span>, given by <spanclass="math inline">\(f:{\Bbb{R}}^n \mapsto \Bbb{R}^n\)</span>, isinvertible such that <span class="math inline">\(X = f(Z)\)</span> and<span class="math inline">\(Z = f^{−1}(X)\)</span>. <spanclass="math display">\[p_{X}(x) = p_{Z} (f^{-1}(x)) |\det(\frac{\partial f^{-1} (x)}{\partial x})|\]</span></li><li><span class="math inline">\(x, z\)</span> need to be continuous andhave the same dimension.</li><li>For any invertible matrix <span class="math inline">\(A\)</span>,<span class="math inline">\(\det(A^{−1}) = \det(A)^{−1}\)</span> <spanclass="math display">\[p_{X}(x) = p_{Z} (z) |\det (\frac{\partial f(z)}{\partial z})|^{-1}\]</span></li></ul><p>In a <strong>normalizing flow model</strong>, the mapping between<span class="math inline">\(Z\)</span> and <spanclass="math inline">\(X\)</span>, given by <spanclass="math inline">\(f_{\theta}:{\Bbb{R}}^n \mapsto \Bbb{R}^n\)</span>,is deterministic and invertible such that <span class="math inline">\(X=f_{\theta}(Z)\)</span> and <span class="math inline">\(Z =f^{−1}_{\theta}(X)\)</span>. the marginal likelihood p(x) is given by<span class="math display">\[p_{X}(x;\theta) = p_{Z} (f^{-1}_\theta(x))|\det (\frac{\partial f_\theta^{-1} (x)}{\partial x})|\]</span></p><p><strong>Normalizing:</strong> Change of variables gives a normalizeddensity after applying an invertible transformation<strong>Flow:</strong> Invertible transformations can be composed witheach other <span class="math display">\[z_m = f^m_\theta \circ \cdots\circ f^1_\theta (z_0) = f^m_\theta (f^{m-1}_\theta (\cdots (f^1_\theta(z_0)))) \triangleq f_\theta (z_0)\]</span></p><ul><li>Start with a simple distribution for <spanclass="math inline">\(z_0\)</span> (e.g., Gaussian)</li><li>Apply a sequence of M invertible transformations to finally obtain<span class="math inline">\(x = z_M\)</span></li><li>By change of variables <span class="math display">\[p_{X}(x;\theta)= p_{Z} (f^{-1}_\theta(x)) \prod^M_{m=1} |\det (\frac{\partial({f^m_\theta})^{-1} (z_m)}{\partial z_m})|\]</span></li></ul><h2 id="learning-and-inference">Learning and Inference</h2><p>Learning via <strong>maximum likelihood</strong> over the dataset<span class="math inline">\(\mathcal{D}\)</span> <spanclass="math display">\[\max_\theta \log p_{X}(\mathcal{D};\theta) =\sum_{x \in \mathcal{D}} \log p_{Z} (f^{-1}_\theta(x)) + \log |\det(\frac{\partial f^{-1}_\theta (x)}{\partial x})|\]</span> <strong>Exactlikelihood evaluation</strong> via inverse tranformation <spanclass="math inline">\(x \mapsto z\)</span> and change of variablesformula <strong>Sampling</strong> via forward transformation <spanclass="math inline">\(z \mapsto x\)</span> <spanclass="math display">\[z \backsim p_{Z} (z) \quad x = f_{\theta}(z)\]</span> <strong>Latent representations</strong> inferred viainverse transformation (no inference network required!) <spanclass="math display">\[z = f^{-1}_{\theta} (x)\]</span></p><p>Computing likelihoods also requires the evaluation of determinants ofn × n Jacobian matrices, where n is the data dimensionality: <spanclass="math inline">\(O(n^3)\)</span> =&gt; <strong>Key idea:</strong>Choose tranformations so that the resulting Jacobian matrix has specialstructure. For example, the determinant of a triangular matrix is theproduct of the diagonal entries, i.e., an O(n) operation.</p><h2 id="invertible-transformations-with-diagonal-jacobians">Invertibletransformations with diagonal Jacobians</h2><h3 id="nonlinear-independent-components-estimation-nice">NonlinearIndependent Components Estimation (NICE)</h3><ol type="1"><li>Additive coupling layers Partition the variables <spanclass="math inline">\(z\)</span> into two disjoint subsets, say <spanclass="math inline">\(z_{1:d}\)</span> and <spanclass="math inline">\(z_{d+1:n}\)</span> for any <spanclass="math inline">\(1 ≤ d &lt; n\)</span><ul><li>Forward mapping <span class="math inline">\(z \mapsto x\)</span><ul><li><span class="math inline">\(x_{1:d} = z_{1:d}\)</span> (identitytransformation)</li><li><span class="math inline">\(x_{d+1:n} = z_{d+1:n} + m_\theta(z_{1:d})\)</span> (<span class="math inline">\(m_\theta(\cdot)\)</span> is a neural network with parameters <spanclass="math inline">\(\theta\)</span>, <spanclass="math inline">\(d\)</span> input units, and <spanclass="math inline">\(n − d\)</span> output units)</li></ul></li><li>Inverse mapping <span class="math inline">\(x \mapsto z\)</span><ul><li><span class="math inline">\(z_{1:d} = x_{1:d}\)</span> (identitytransformation)</li><li><span class="math inline">\(z_{d+1:n} = x_{d+1:n} - m_\theta(x_{1:d})\)</span></li></ul></li><li>Jacobian of forward mapping $$ J = (<span class="math display">\[\begin{matrix} I_d, 0 \\ \frac{\partialx_{d+1:n}}{\partial z_{1:d}} , I_{n-d} \end{matrix}\]</span>) \ (J) = 1 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    - **Volume preserving transformation** since determinant is 1.</span><br><span class="line"></span><br><span class="line">2. Rescaling layers</span><br><span class="line">    - Additive coupling layers are composed together (with arbitrary partitions of variables in each layer)</span><br><span class="line">    - Final layer of NICE applies a rescaling transformation</span><br><span class="line">    - Forward mapping $z \mapsto x$:</span><br><span class="line">    $$    x_i = s_i z_i</span><br></pre></td></tr></table></figure> where <span class="math inline">\(s_i &gt;0\)</span> is the scaling factor for the i-th dimension.</li><li>Inverse mapping <span class="math inline">\(x \mapsto z\)</span>: $$z_i = <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- Jacobian of forward mapping:</span><br><span class="line">$$    J = \textrm&#123;diag&#125; (s) \\</span><br><span class="line">\det (J) = \prod^n_&#123;i=1&#125; s_i</span><br></pre></td></tr></table></figure></li></ul></li></ol><h3 id="real-nvp-non-volume-preserving-extension-of-nice">Real-NVP:Non-volume preserving extension of NICE</h3><ul><li>Forward mapping <span class="math inline">\(z \mapsto x\)</span><ul><li><span class="math inline">\(x_{1:d} = z_{1:d}\)</span> (identitytransformation)</li><li><span class="math inline">\(x_{d+1:n} = z_{d+1:n} \odot \exp(\alpha_\theta (z_{1:d})) + \mu_{\theta} (z_{1:d})\)</span></li><li><span class="math inline">\(\mu_\theta (\cdot),\alpha_\theta\)</span> are both neural network with parameters <spanclass="math inline">\(\theta\)</span> <spanclass="math inline">\(d\)</span> input units, and <spanclass="math inline">\(n − d\)</span> output units, <spanclass="math inline">\(\odot\)</span> denotes elementwise product</li></ul></li><li>Inverse mapping <span class="math inline">\(x \mapsto z\)</span><ul><li><span class="math inline">\(z_{1:d} = x_{1:d}\)</span> (identitytransformation)</li><li><span class="math inline">\(z_{d+1:n} = (x_{d+1:n} - \mu_\theta(x_{1:d})) \odot (\exp (-\alpha_\theta (x_{1:d})))\)</span></li></ul></li><li>Jacobian of forward mapping <span class="math display">\[J =\frac{\partial x}{\partial z} \left( \begin{matrix} I_d, 0 \\\frac{\partial x_{d+1:n}}{\partial z_{1:d}} , \textrm{diag}(\exp(\alpha_{\theta (z_{1:d})})) \end{matrix} \right) \\\det (J) = \prod^n_{i=d+1} \exp (\alpha_\theta (z_{1:d})_i) = \exp(\sum^n_{i=d+1} \alpha_\theta (z_{1:d})_i)\]</span></li><li><strong>Non-volume preserving transformation</strong> in generalsince determinant can be less than or greater than 1</li></ul><h2 id="autoregressive-models-as-normalizing-flow-models">AutoregressiveModels as Normalizing Flow Models</h2><ul><li><p>Consider a Gaussian autoregressive model: $$ p(x) = ^n_{i=1}p(x_i | x_{&lt;i}) <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line">    such that $p(x_i|x_&#123;&lt;i&#125;)=\mathcal&#123;N&#125; (\mu_i (x_1,...,x_&#123;i-1&#125;), \exp (\alpha_i (x_1,...,x_&#123;i-1&#125;))^2)$.$\mu_i (\cdot), \alpha_i (\cdot)$ are neural networks for $i &gt; 1$ and constants for $i = 1$. </span><br><span class="line">- Sampler for this model:</span><br><span class="line">    - Sample $z_i \backsim \mathcal&#123;N&#125; (0,1)$ for $i=1,...,n$</span><br><span class="line">    - Let $x_1 = \exp (\alpha_1) z_1 + \mu_1$. Compute $\mu_2(x_1),\alpha_2 (x_1)$</span><br><span class="line">    - Let $x_2 = \exp (\alpha_2) z_2 + \mu_2$. Compute $\mu_3(x_1,x_2),\alpha_3 (x_1,x_2)$</span><br><span class="line">    - Let $x_3 = \exp (\alpha_3) z_3 + \mu_3$...</span><br><span class="line">- **Flow interpretation:** transforms samples from the standard Gaussian $(z_1,z_2,...,z_n)$ to those generated from the model $(x_1,x_2,...,x_n)$ via invertible transformations (parameterized by $\mu_i (\cdot), \alpha_i (\cdot)$</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Masked Autoregressive Flow (MAF)</span><br><span class="line"></span><br><span class="line">[![fwd.png](https://s1.ax1x.com/2023/02/13/pSIWBi8.md.png)](https://imgse.com/i/pSIWBi8)</span><br><span class="line"></span><br><span class="line">- Forward mapping $z \mapsto x$</span><br><span class="line">    - Let $x_1 = \exp (\alpha_1)z_1 + \mu_1$. Compute $\mu_2(x_1), \alpha_2(x_1)$</span><br><span class="line">    - Let $x_2 = \exp (\alpha_2)z_2 + \mu_2$. Compute $\mu_3(x_1, x_2), \alpha_3(x_1, x_2)$</span><br><span class="line">- Sampling is sequential and slow (like autoregressive): $\mathcal&#123;O&#125;(n)$ time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[![bwd.png](https://s1.ax1x.com/2023/02/13/pSIWDJS.md.png)](https://imgse.com/i/pSIWDJS)</span><br><span class="line"></span><br><span class="line">- Inverse mapping from $x \mapsto z$:</span><br><span class="line">    - Compute all $\mu_i, \alpha_i$ (can be done in parallel)</span><br><span class="line">    - Let $z_1 = (x_1 - \mu_1) / \exp (\alpha_1)$ (scale and shift)</span><br><span class="line">    - Let $z_2 = (x_2 - \mu_2) / \exp (\alpha_2)$</span><br><span class="line">    - Let $z_3 = (x_3 - \mu_3) / \exp (\alpha_3)$...</span><br><span class="line">- Jacobian is lower diagonal, hence efficient determinant computation </span><br><span class="line">- Likelihood evaluation is easy and parallelizable (计算$p(x_1),p(x_2|x_1),...,p(x_D|x_&#123;1:D-1&#125;)$时可以并行)</span><br><span class="line">- sampling autoregressive models is slow because you must wait for all previous $x_&#123;1:i−1&#125;$ to be computed before computing new $x_i$</span><br><span class="line">- Layers with different variable orderings can be stacked</span><br><span class="line"></span><br><span class="line">### Inverse Autoregressive Flow (IAF)</span><br><span class="line"></span><br><span class="line">[![IAF.png](https://s1.ax1x.com/2023/02/13/pSIxrMn.md.png)](https://imgse.com/i/pSIxrMn)</span><br><span class="line"></span><br><span class="line">- Forward mapping $z \mapsto x$ (parallel):</span><br><span class="line">    - Sample $z_i \backsim \mathcal&#123;N&#125; (0,1)$ for $i=1,...,n$</span><br><span class="line">    - Compute all $\mu_i, \alpha_i$ (can be done in parallel)</span><br><span class="line">    - Let $x_1 = \exp (\alpha_1) z_1 + \mu_1$</span><br><span class="line">    - Let $x_2 = \exp (\alpha_2) z_2 + \mu_2$...</span><br><span class="line">- Inverse mapping from $x \mapsto z$ (sequential):</span><br><span class="line">    - Let $z_1 = (x_1 − \mu_1)/ \exp (\alpha_1)$. Compute $\mu_2(z_1), \alpha_2(z_1)$</span><br><span class="line">    - Let $z_2 = (x_2 − \mu_2)/ \exp (\alpha_2)$. Compute $\mu_3(z_1,z_2), \alpha_3(z_1,z_2)$</span><br><span class="line">- Fast to sample from, slow to evaluate likelihoods of data points (train)</span><br><span class="line">- Note: Fast to evaluate likelihoods of a generated point (cache $z_1,z_2,..., z_n$)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### IAF vs. MAF</span><br><span class="line"></span><br><span class="line">- Computational tradeoffs</span><br><span class="line">    - MAF: Fast likelihood evaluation, slow sampling </span><br><span class="line">    - IAF: Fast sampling, slow likelihood evaluation</span><br><span class="line">- MAF more suited for training based on MLE, density estimation </span><br><span class="line">- IAF more suited for real-time generation</span><br><span class="line"></span><br><span class="line"># Energy-Based Models (EBMs)</span><br><span class="line"></span><br><span class="line">## Parameterizing probability distributions</span><br><span class="line"></span><br><span class="line">1. non-negative: $p(x) \geq 0$</span><br><span class="line">    - $g_\theta (\bf&#123;x&#125;) =f_\theta(\bf&#123;x&#125;)^2$</span><br><span class="line">    - $g_\theta (\bf&#123;x&#125;) =\exp (f_\theta(\bf&#123;x&#125;))$</span><br><span class="line">    - $g_\theta (\bf&#123;x&#125;) =| f_\theta(\bf&#123;x&#125;)|$</span><br><span class="line">    - $g_\theta (\bf&#123;x&#125;) = \log(1 + \exp ( f_\theta(\bf&#123;x&#125;)))$</span><br><span class="line"></span><br><span class="line">2. sum-to-one: $\sum_x p(x) = 1$ (or $\int p(x) dx = 1$ for continuous variables)</span><br><span class="line">$$p_\theta (\bf&#123;x&#125;) = \frac&#123;1&#125;&#123;\textrm&#123;Volume&#125; (g_\theta)&#125; g_\theta (\bf&#123;x&#125;) = \frac&#123;1&#125;&#123;\int g_\theta (\bf&#123;x&#125;) d\bf&#123;x&#125;&#125; g_\theta (\bf&#123;x&#125;)$$</span><br><span class="line">- Example: choose $g_\theta(\bf&#123;x&#125;)$ so that we know the volume analytically as a</span><br><span class="line">function of $\theta$.</span><br><span class="line">    - $g_&#123;(\mu, \sigma)&#125; (x) = e^&#123;-\frac&#123;(x-\mu)^2&#125;&#123;2\sigma^2&#125;&#125;$. Volume: $\int e^&#123;-\frac&#123;x-\mu&#125;&#123;2\sigma^2&#125;&#125;dx=\sqrt&#123;2 \pi \sigma^2&#125;$. (Gaussian)</span><br><span class="line">    - $g_\lambda (x) = e^&#123;-\lambda x&#125;$. Volume: $\int^&#123;+ \infty&#125;_0 e^&#123;- \lambda x&#125; dx=\frac&#123;1&#125;&#123;\lambda&#125;$. (Exponential)</span><br><span class="line">    - $g_\theta (x) = h(x) \exp \&#123; \theta \cdot T(x)\&#125;$. Volume: $\exp \&#123; A (\theta)\&#125;$ where $A(\theta)=\log \int h(x) \exp \&#123;\theta \cdot T(x) \&#125; dx$. (Exponential family)</span><br><span class="line">        - Normal, Poisson, exponential, Bernoulli</span><br><span class="line">        - beta, gamma, Dirichlet, Wishart, etc.</span><br><span class="line"></span><br><span class="line">## Likelihood based learning</span><br><span class="line"></span><br><span class="line">More complex models can be obtained by combining these building blocks.</span><br><span class="line"></span><br><span class="line">1. Autoregressive: Products of normalized objects $p_\theta (\bf&#123;x&#125;) p_&#123;\theta^&#123;&#x27;&#125;(\bf&#123;x&#125;)&#125; (\bf&#123;y&#125;)$:</span><br><span class="line">$$\int_x \int_y p_\theta (\bf&#123;x&#125;) p_&#123;\theta^&#123;&#x27;&#125;(\bf&#123;x&#125;)&#125; (\bf&#123;y&#125;) d \bf&#123;x&#125; d\bf&#123;y&#125; = \int_x p_\theta (\bf&#123;x&#125;) \underbrace&#123;\int_y p_&#123;\theta^&#123;&#x27;&#125;&#125; (\bf&#123;y&#125;) d \bf&#123;y&#125; &#125;_&#123;=1&#125; d \bf&#123;x&#125;=\int_x p_&#123;\theta&#125; (\bf&#123;x&#125;) d\bf&#123;x&#125; = 1$$ </span><br><span class="line"></span><br><span class="line">2. Latent variables: Mixtures of normalized objects $\alpha p_\theta (\bf&#123;x&#125;) + (1-\alpha) p_&#123;\theta^&#123;&#x27;&#125;&#125; (\bf&#123;x&#125;)$</span><br><span class="line">$$\int_x \alpha p_&#123;\theta&#125; (\bf&#123;x&#125;) + (1- \alpha) p_&#123;\theta^&#123;&#x27;&#125;&#125; (\bf&#123;x&#125;) d \bf&#123;x&#125; = \alpha + (1-\alpha) = 1$$</span><br><span class="line">## Energy-based model</span><br><span class="line"></span><br><span class="line">$$p_\theta(\bf&#123;x&#125;) = \frac&#123;1&#125;&#123;\int \exp (f_\theta (\bf&#123;x&#125;))d \bf&#123;x&#125;&#125; \exp (f_\theta (\bf&#123;x&#125;)) = \frac&#123;1&#125;&#123;Z(\theta)&#125; \exp (f_\theta (\bf&#123;x&#125;)) $$</span><br><span class="line"></span><br><span class="line">The volume/normalization constant (partition function)</span><br><span class="line"></span><br><span class="line">$$Z(\theta) = \int \exp (f_\theta (\bf&#123;x&#125;))d \bf&#123;x&#125;$$</span><br><span class="line"></span><br><span class="line">- $-f_\theta(\bf&#123;x&#125;)$ is called the energy</span><br><span class="line">- Intuitively, configurations $\bf&#123;x&#125;$ with low energy (high $f_\theta(\bf&#123;x&#125;)$) are more likely.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Pros:</span><br><span class="line">- Very flexible model architectures.</span><br><span class="line">- Stable training.</span><br><span class="line">- Relatively high sample quality.</span><br><span class="line">- Flexible composition.</span><br><span class="line"></span><br><span class="line">Cons:</span><br><span class="line">- Sampling from $p_\theta (\bf&#123;x&#125;)$ is hard</span><br><span class="line">- Evaluating and optimizing likelihood $p_\theta (\bf&#123;x&#125;)$ is hard (learning is hard)</span><br><span class="line">- No feature learning (but can add latent variables)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**Curse of dimensionality**: The fundamental issue is that computing $Z(\theta)$ numerically (when no analytic solution is available) scales exponentially in the number of dimensions of $\bf&#123;x&#125;$.</span><br><span class="line"></span><br><span class="line">Nevertheless, some tasks do not require knowing $Z(\theta)$.</span><br><span class="line">Given a trained model, many applications require relative comparisons. Hence $Z(\theta)$ is not needed.</span><br><span class="line">Comparing the probability of two points is easy: $p_\theta (\bf&#123;x&#125;&#x27;)/p_\theta (\bf&#123;x&#125;) = \exp (f_\theta (\bf&#123;x&#125;&#x27;) - f_\theta (\bf&#123;x&#125;))$</span><br><span class="line"></span><br><span class="line">## Sampling from energy-based models</span><br><span class="line"></span><br><span class="line">- Maximum likelihood training: $\max_\theta&#123;f_\theta(x_&#123;\textrm&#123;train&#125;&#125;)−\log Z(\theta)&#125;$.</span><br><span class="line">    - Contrastive divergence: </span><br><span class="line">    $$    \nabla_\theta f_\theta (\bf&#123;x&#125;_&#123;\textrm&#123;train&#125;&#125;) - \nabla_\theta \log Z(\theta) \approx \nabla_\theta f_\theta (\bf&#123;x&#125;_&#123;\textrm&#123;train&#125;&#125;) - \nabla_\theta f_\theta(\bf&#123;x&#125;_&#123;\textrm&#123;sample&#125;&#125;)</span><br></pre></td></tr></table></figure> where <spanclass="math inline">\(\bf{x}_{\textrm{sample}} \backsim p_\theta(\bf{x})\)</span>.</p></li><li><p>Unadjusted Langevin MCMC</p><ul><li><span class="math inline">\(\bf{x}^0 \backsim \pi(\bf{x})\)</span></li><li>Repeat for <span class="math inline">\(t=0,1,2,\cdots,T-1\)</span><ul><li><span class="math inline">\(\bf{z}^t \backsim \mathcal{N}(0,I)\)</span></li><li><span class="math inline">\(\bf{x}^{t+1} = \bf{x}^t + \epsilon\nabla_x \log p_\theta (\bf{x})|_{\bf{x}=\bf{x}^t} + \sqrt{2 \epsilon}\bf{z}^t\)</span></li></ul></li></ul></li><li><p>Properties:</p><ul><li><span class="math inline">\(\bf{x}^{\top}\)</span> converges to<span class="math inline">\(p_\theta(\bf{x})\)</span> when <spanclass="math inline">\(T \rightarrow \infty\)</span> and <spanclass="math inline">\(\epsilon \rightarrow 0\)</span>.</li><li><span class="math inline">\(\nabla_x \log p_\theta (\bf{x}) =\nabla_x f_\theta (\bf{x})\)</span> for continuous energy-basedmodels.</li><li>Convergence slows down as dimensionality grows.</li></ul></li></ul><h2 id="training-without-sampling">Training without sampling</h2><h3 id="score-function">Score Function</h3><p><span class="math display">\[s_\theta (\bf{x}) := \nabla_x \logp_\theta (\bf{x}) = \nabla_x f_\theta (\bf{x}) - \underbrace{\nabla_x\log Z(\theta)}_{=0} = \nabla_x f_\theta (\bf{x}) \]</span></p><p><span class="math inline">\(s_\theta (\bf{x})\)</span> is independentof the partition function <spanclass="math inline">\(Z(\theta)\)</span>.</p><h3 id="score-matching">Score Matching</h3><p>Minimizing the Fisher divergence between <spanclass="math inline">\(p_{\textrm{data}}(\bf{x})\)</span> and the EBM<span class="math inline">\(p_\theta(\bf{x})\)</span></p><p><span class="math display">\[\frac{1}{2} E_{x \backsimp_{\textrm{data}}} [|| \nabla_x \log p_{\textrm{data}}(\bf{x}) -s_\theta (\bf{x}) ||^2_2] \\= \frac{1}{2} E_{x \backsim p_{\textrm{data}}} [|| \nabla_x \logp_{\textrm{data}}(\bf{x}) - \nabla_x f_\theta (\bf{x})||^2_2]\]</span></p><ol type="1"><li>Sample a mini-batch of datapoints <spanclass="math inline">\(\{\bf{x}_1, \bf{x}_2,\cdots, \bf{x}_n \} \backsimp_{\textrm{data}} (\bf{x})\)</span>.</li><li>Estimate the score matching loss with the empirical mean <spanclass="math display">\[\frac{1}{n} \sum^n_{i=1} [\frac{1}{2} || \nabla_x\log p_\theta (\bf{x}_i) ||^2_2 + \textrm{tr} (\nabla^2_x \log p_\theta(\bf{x}_i)) ] \\= \frac{1}{n} \sum^n_{i=1} [\frac{1}{2} || \nabla_x f_\theta (\bf{x}_i)||^2_2 + \textrm{tr} (\nabla^2_x f_\theta (\bf{x}_i)) ]\]</span></li><li>Stochastic gradient descent.</li></ol><h1 id="summary">Summary</h1><ul><li>Autoregressive models. <span class="math inline">\(p_\theta(x_1,x_2,\cdots, x_n) = \prod^n_{i=1} p_\theta(x_i | x&lt;i)\)</span></li><li>Normalizing flow models. <spanclass="math inline">\(p_\theta(\bf{x}) = p(\bf{z})| \det J_{f_θ}(\bf{x})|\)</span>, where <span class="math inline">\(\bf{z} =f_θ(\bf{x})\)</span></li><li>Variational autoencoders: <spanclass="math inline">\(p_\theta(\bf{x}) = \int p(\bf{z}) p_{\theta}(\bf{x}|\bf{z}) d\bf{z}\)</span>.</li><li>Energy-based models: <span class="math inline">\(p_\theta (\bf{x}) =\frac{e^{f_\theta}}{Z(\theta)}\)</span></li><li>Pros:<ul><li>Maximum likelihood training</li><li>Principled model comparison via likelihoods</li></ul></li><li>Cons:<ul><li>Model architectures are restricted.</li><li>Special architectures or surrogate losses to deal with intractablepartition functions</li></ul></li><li>Generative Adversarial Networks (GANs).<ul><li>$ <em></em>E_{x∼p_{}} [D_(]$.</li></ul></li><li>Pros:<ul><li>Two sample tests. Can optimize f-divergences and the Wassersteindistance.</li><li>Very flexible model architectures.</li><li>Samples typically have better quality</li></ul></li><li>Cons:<ul><li>Require adversarial training. Training instability and modecollapse.</li><li>No principled way to compare different models</li><li>No principled termination criteria for training</li></ul></li></ul><h1 id="reference">Reference</h1><p>[1] 宏观理解Diffusion过程: <ahref="https://kexue.fm/archives/9119">DDPM = 拆楼 + 建楼</a> [2]DDPM原论文遵循Diffusion-Original: <ahref="https://kexue.fm/archives/9152">DDPM = 自回归式VAE</a> [3] 和scorematching串联，和DDIM延续: <a href="https://kexue.fm/archives/9164">DDPM= 贝叶斯 + 去噪</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Understand and create complex, unstructured data.&lt;/p&gt;</summary>
    
    
    
    <category term="survey" scheme="http://yoursite.com/categories/survey/"/>
    
    
    <category term="Probabilistic Model" scheme="http://yoursite.com/tags/Probabilistic-Model/"/>
    
    <category term="Generative Model" scheme="http://yoursite.com/tags/Generative-Model/"/>
    
  </entry>
  
  <entry>
    <title>Summary of Self-Supervised Learning</title>
    <link href="http://yoursite.com/2022/05/03/Summary-of-Self-Supervised-Learning/"/>
    <id>http://yoursite.com/2022/05/03/Summary-of-Self-Supervised-Learning/</id>
    <published>2022-05-03T11:31:22.000Z</published>
    <updated>2023-03-24T09:11:50.748Z</updated>
    
    <content type="html"><![CDATA[<p>近期自监督学习论文总结。</p><span id="more"></span><h1id="moco-momentum-contrast-for-unsupervised-visual-representation-learning-cvpr-2020">MoCo(Momentum Contrast for Unsupervised Visual Representation Learning, CVPR2020)</h1><h2 id="main-idea">Main idea</h2><p>采用对比学习的方式学习图片表征，其核心是引入了具有队列的动态字典以及移动平均编码器。</p><ol type="1"><li>它将dict size与mini-batchsize解耦，将队列的大小作为超参数，而字典中的样本将被渐进替换。</li><li>此外，若直接把encoder q中的参数拷贝到encoderk中，快速变化的encoder将降低表示的一致性。因此，该工作采取了momentumupdate的方式，仅对encoder k进行少量更新。</li></ol><h2 id="method">Method</h2><p><a href="https://imgtu.com/i/IlFJGd"><imgsrc="https://z3.ax1x.com/2021/11/06/IlFJGd.png"alt="IlFJGd.png" /></a></p><p>InFoNCE: 选取<span class="math inline">\([k_1,...,k_{K+1}]\)</span>,含一个正样本<span class="math inline">\(k_+\)</span>, K个负样本<spanclass="math inline">\(k_i\)</span> (其他随便信号)。</p><p><span class="math display">\[\mathcal{L}_q = - \rm{log}\frac{\rm{exp}(q \cdot k_{+} / \tau)}{\sum^{K}_{i=0} \exp (q \cdot k_i /\tau)}\]</span></p><p>Momentum Update</p><p><span class="math display">\[\theta_k \leftarrow m \theta_k + (1-m)\theta_q\]</span></p><h1id="byol-bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-nips-2020">BYOL(Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,NIPS 2020)</h1><h2 id="problem">Problem</h2><p>现有的图片表示对比学习将同一图片的不同增强视图作为正样本，通过约束同一张图的不同形态之间的特征差异性来实现特征提取（一般通过数据增强实现）。若仅有正样本，网络容易对所有输入都输出一个固定的值，这样特征差异性就是0，完美符合优化目标，但这不是我们想要的，即导致了Collapsing。</p><p>负样本对解决了训练崩塌的问题，但对数量要求较大，因为只有这样才能训练出足够强的特征提取能力，因此往往需要较大的batchsize才能有较好的效果。</p><p>BYOL无负样本对，通过增加prediction和stop-gradient避免训练退化。</p><h2 id="motivation">Motivation</h2><p>作者首先展示了一个实验，一个网络参数随机初始化且固定的targetnetwork的top1准确率只有1.4%，而targetnetwork输出feature作为另一个叫online network的训练目标，等这个onlinenetwork训练好之后，online network的top1准确率可以达到18.8%。</p><p>假如将target network替换为效果更好的网络参数（比如此时的onlinenetwork），然后再迭代一次，也就是再训练一轮onlinenetwork，去学习新的targetnetwork输出的feature，那效果应该是不断上升的，类似左右脚踩楼梯不断上升一样。</p><h2 id="method-1">Method</h2><p><a href="https://imgtu.com/i/5XeZOH"><imgsrc="https://z3.ax1x.com/2021/10/29/5XeZOH.png"alt="5XeZOH.png" /></a></p><p>给定一个target network产生表示，用一个onlinenetwork去预测target表示。若target与online同步更新，表现很差；若保持target不变，acc约18.8%。因此对target采用EMA（ExponentialMoving Average）策略。</p><p>给定a set of weights of online network <spanclass="math inline">\(\theta\)</span>, a set of weight of target network<span class="math inline">\(\xi\)</span>, target decay rate <spanclass="math inline">\(\tau \in [0,1]\)</span>,在训练之后进行以下更新：</p><p><span class="math display">\[\xi \leftarrow \tau \xi + (1- \tau)\theta\]</span></p><p>A set of images: <span class="math inline">\(\mathcal{D}\)</span>, animage <span class="math inline">\(x~\mathcal{D}\)</span> sampleduniformly. 图像增强分布<span class="math inline">\(\mathcal{T}\)</span>and <span class="math inline">\(\mathcal{T}\)</span>.<br /><span class="math inline">\(v \triangleq t(x)\)</span> and <spanclass="math inline">\(v&#39; \triangleq t&#39;(x)\)</span>: twoaugmented views from <span class="math inline">\(x\)</span>.</p><p><span class="math inline">\(y_{\theta} \triangleq f_{\theta}(v)\)</span>: online network产生的表示<br /><span class="math inline">\(z_{\theta} \triangleq g_{\theta}(y)\)</span>: projection<br /><span class="math inline">\(q_{\theta}(z_{\theta})\)</span>: predictionof <span class="math inline">\(z&#39;_{\xi}\)</span><br />分别对<span class="math inline">\(q_{\theta}(z_{\theta})\)</span>和<spanclass="math inline">\(z&#39;_{\xi}\)</span>进行l2 归一化： <spanclass="math inline">\(\overline{q}_{\theta}(z_{\theta}) \triangleqq_{\theta}(z_{\theta}) / || q_{\theta}(z_{\theta}) ||_2\)</span>, <spanclass="math inline">\(\overline{z}_{\theta} \triangleq z&#39;_{\xi} /||z&#39;_{\xi}||_2\)</span></p><p>仅有online有predictor，两个branch之间是非对称的。</p><p>计算online的prediction和target的projection的MSE。</p><p><span class="math display">\[\mathcal{L}_{\theta , \xi} \triangleq ||\overline{q_{\theta}} - \overline{z_{\xi}}&#39; ||_2^2 = 2-2 \cdot\frac{&lt;q_{\theta}, z&#39;_{\xi}&gt;}{||q_{\theta}||_2 \cdot||z&#39;_{\xi}||_2}\]</span></p><p>分别将<span class="math inline">\(v&#39;\)</span>输入onlinenetwork、<span class="math inline">\(v\)</span>输入targetnetwork计算<span class="math inline">\(\widetilde{\mathcal{L}}_{\theta ,\xi}\)</span>，使得loss <span class="math inline">\(\mathcal{L}_{\theta, \xi}\)</span>对称。</p><p>在每个训练步进行随机优化，针对<spanclass="math inline">\(\theta\)</span>最小化<spanclass="math inline">\(\mathcal{L}^{BYOL}_{\theta,\xi}=\mathcal{L}_{\theta , \xi} + \widetilde{\mathcal{L}}_{\theta ,\xi}\)</span>,而<span class="math inline">\(\xi\)</span>则采取stopgradient策略。</p><p><span class="math display">\[\theta \leftarrow \rm{optimizer}(\theta,\nabla_{\theta} \mathcal{L}^{BYOL}_{\theta, \xi}, \eta) \xi \leftarrow\tau \xi + (1-\tau) \theta\]</span> 其中<spanclass="math inline">\(\eta\)</span>是个可学习参数。</p><h1id="albef-align-before-fuse-vision-and-language-representation-learning-with-momentum-distillation-nips-2021">ALBEF(Align before Fuse: Vision and Language Representation Learning withMomentum Distillation, NIPS 2021)</h1><h2 id="main-idea-1">Main idea</h2><p>大多数现有基于transformer作为多模编码器的工作共同建模visualtokens和wordtokens，然而token之间是非对齐的，往往难以捕捉它们之间的交互。 本文提出，1. 采用对比损失在cross-modal attention fusing之前对图像和文本进行对齐。2. 为了应对噪声，提出momentum distillation，从momentummodel所产生的伪标签中学习。</p><h2 id="method-2">Method</h2><p><a href="https://imgtu.com/i/5XeMkt"><imgsrc="https://z3.ax1x.com/2021/10/29/5XeMkt.png"alt="5XeMkt.png" /></a></p><h3 id="pre-training-objectives">Pre-training Objectives</h3><h4 id="image-text-contrastive-learning">Image-Text ContrastiveLearning</h4><p>对比损失旨在在融合之前学习一个更好的单模表示。</p><p>需要学习一个similarity function <span class="math inline">\(s=g_v(\bold{v}_{cls})^{\top} g_w(\bold{w}_{cls})\)</span>。受MoCo启发，维护了两个队列来保存由momentumunimodal encoders产生的最近的M个图片-文本表示。由momentumencoders所产生的归一化的特征表示为<span class="math inline">\(g&#39;_v(\bold{v}_{cls}&#39;)\)</span>和<span class="math inline">\(g&#39;_w(\bold{w}_{cls}&#39;)\)</span>。 我们定义<spanclass="math inline">\(s(I,T)=g_v (\bold{v}_{cls})^{\top}g_w&#39;(\bold{w}_{cls}&#39;)\)</span>, <span class="math inline">\(s(T,I)=g_w(\bold{w}_{cls})^{\top}g_v&#39; (\bold{v}_{cls}&#39;)\)</span>.</p><p>calculate the softmax-normalized image-to-text and text-to-imagesimilarity as:</p><p><span class="math display">\[p^{i2t}_m (I)=\frac{\exp (s(I,T_m)/\tau)}{\sum_{m=1}^{M} \exp (s(I, T_m)/\tau)},p^{t2i}_m (T)=\frac{\exp (s(T,I_m)/ \tau)}{\sum_{m=1}^{M} \exp (s(T,I_m)/\tau)}\]</span></p><p><span class="math inline">\(\tau\)</span>: learnable temperatureparameter</p><p><span class="math inline">\(\bold{y}^{i2t}(I)\)</span> and <spanclass="math inline">\(\bold{y}^{t2i}(T)\)</span>: ground-truth one-hotsimilarity</p><p>对比损失被定义为<span class="math inline">\(\bold{p}\)</span>和<spanclass="math inline">\(\bold{y}\)</span>的交叉熵。</p><p><span class="math display">\[\mathcal{L}_{itc} = \frac{1}{2}\rm{E}_{(I,T) \backsim D} [\rm{H}(\bold{p}^{i2t}(I), \bold{y}^{i2t}(I))+ \rm{H}(\bold{p}^{t2i}(T), \bold{y}^{t2i}(T))]\]</span></p><h4 id="masked-language-modeling">Masked Language Modeling</h4><p>利用图片和上下文文本信息去预测masked words。</p><p><span class="math inline">\(\hat{T}\)</span>: masked text<br /><span class="math inline">\(\bold{p}^{msk} (I, \hat{T})\)</span>:model’s predicted probability for a masked token.</p><p>MLM 最小化交叉熵:</p><p><spanclass="math display">\[\mathcal{L}_{mlm}=\rm{E}_{(I,\hat{T})\backsim D}\rm{H} (\bold{p}^{msk} (I, \hat{T}, \bold{y}^{msk})\]</span> <spanclass="math inline">\(\bold{y}^{msk}\)</span>: one-hot vocabularydistribution,真实token的概率是1</p><h4 id="image-text-matching">Image-Text Matching</h4><p>预测一组图文对为正（匹配）或负（不匹配）。仅采用[CLS]token的embedding。</p><p><span class="math display">\[\mathcal{L}_{itm} = \rm{E}_{(I,T)\backsim D} \rm{H}(\bold{p}^{itm}(I, T), \bold{y}^{itm})\]</span><spanclass="math inline">\(\bold{y}^{itm}\)</span>: 2维one-hot ground-truthlabel向量表示</p><p><span class="math display">\[\mathcal{L} = \mathcal{L}_{itc} +\mathcal{L}_{mlm} + \mathcal{L}_{itm}\]</span></p><h3 id="momentum-distillation">Momentum Distillation</h3><p>正样本对往往弱相关，也会包含一些不相关的文本，或实体，而对于ITC学习来说，负文本可能也会包含一些匹配图片的信息。因此采用一个动量模型，它是一个exponential-moving-averageversions of the unimodal and multimodal encoders。</p><p>For ITC,首先利用动量单模encoder产生的特征计算相似度：<spanclass="math inline">\(s&#39;(I,T)=g_v&#39;(\bold{v}_{cls}&#39;)^{\top}g_w&#39;(\bold{w}_{cls}&#39;)\)</span>以及<spanclass="math inline">\(s&#39;(T,I)=g_w&#39;(\bold{w}_{cls}&#39;)^{\top}g_v&#39;(\bold{v}_{cls}&#39;)\)</span></p><p>之后，计算soft pseudo-targets<spanclass="math inline">\(\bold{q}^{i2t}\)</span>和<spanclass="math inline">\(\bold{q}^{t2i}\)</span>,计算损失：</p><p><span class="math display">\[\mathcal{L}^{\rm{mod}}_{\rm{itc}} =(1-\alpha) \mathcal{L}_{itc} + \frac{\alpha}{2} \rm{E}_{(I,T) \backsimD} [\rm{KL}(\bold{p}^{i2t}(I), \bold{q}^{i2t}(I)) +\rm{KL}(\bold{p}^{t2i}(T), \bold{q}^{t2i}(T))]\]</span></p><p>For MLM, <spanclass="math inline">\(\bold{q}^{msk}(I,\hat{T})\)</span>表示momentummodel’s prediction probability for the masked token, 损失表示为：</p><p><span class="math display">\[\mathcal{L}^{\rm{mod}}_{\rm{mlm}} =(1-\alpha) \mathcal{L}_{mlm} + {\alpha} \rm{E}_{(I,\hat{T}) \backsim D}\rm{KL}(\bold{p}^{msk}(I,\hat{T}),\bold{q}^{msk}(I,\hat{T}))\]</span></p><h1 id="barlow-twins">Barlow Twins</h1><p><a href="https://imgtu.com/i/5XebBd"><imgsrc="https://z3.ax1x.com/2021/10/29/5XebBd.png"alt="5XebBd.png" /></a></p><p>将样本经过不同增广送入同一网络中得到两种表示，利用损失函数迫使它们的互相关矩阵接近于恒等矩阵：这意味着同一样本不同的增广版本下提取出的特征表示非常类似，同时特征向量分量间的冗余最小化。</p><p>损失函数如下所示，C为互相关矩阵，其中invarianceterm使得对角元素接近于1，促使同一样本在不同失真版本下的特征一致性，redundancyreduction term使非对角元素接近0，解耦特征表示的不同向量分量</p><figure><img src="https://img-blog.csdnimg.cn/20210320234759199.png"alt="损失函数" /><figcaption aria-hidden="true">损失函数</figcaption></figure><p>文中还给出了互相关矩阵C的详细计算公式，如下所示：</p><figure><img src="https://img-blog.csdnimg.cn/20210320234825598.png"alt="互相关矩阵" /><figcaption aria-hidden="true">互相关矩阵</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;近期自监督学习论文总结。&lt;/p&gt;</summary>
    
    
    
    <category term="survey" scheme="http://yoursite.com/categories/survey/"/>
    
    
    <category term="Self-supervised Learning" scheme="http://yoursite.com/tags/Self-supervised-Learning/"/>
    
    <category term="Contrasive Learning" scheme="http://yoursite.com/tags/Contrasive-Learning/"/>
    
  </entry>
  
  <entry>
    <title>From CNN to GCNN</title>
    <link href="http://yoursite.com/2022/05/03/From-CNN-to-GCNN/"/>
    <id>http://yoursite.com/2022/05/03/From-CNN-to-GCNN/</id>
    <published>2022-05-03T09:49:46.000Z</published>
    <updated>2023-03-24T09:17:16.170Z</updated>
    
    <content type="html"><![CDATA[<p>CNN无法处理非欧式数据，又希望在拓扑图上有效地提取空间特征来进行机器学习。因此GCNN成为了研究重点。</p><span id="more"></span><h1 id="cnn的性质">CNN的性质</h1><p>CNN具有局部平稳(i.e.与位置无关，满足平移不变形)以及多尺度的特点。</p><p>在图像领域，数据以格子的形式分布，非常规律，对其进行处理时需要处理的是每个格子上的信号而非结构。</p><p>然而，CNN仅在欧式数据上能够发挥作用，对于图、流形等非欧式数据而言，需要进行一定的变换。</p><p>欧式数据的特点： 1. 非负 2. 对称 3. 满足三角不等式</p><h1 id="傅里叶变换">傅里叶变换</h1><p>在连续的空间上：</p><p><span class="math display">\[h(t) = (f * g)(t)\overset{def}\Longrightarrow \int f(t)g(t - \tau) d \tau\]</span></p><p>在离散的空间上：</p><p><span class="math display">\[h(x,y) = (f * g)(x,y)\overset{def}\Longrightarrow \sum_{m,n} f(x-m,y-n)g(m,n)\]</span></p><p>不同域的CNN:</p><ul><li>对于频域而言，卷积核若定义在频域，则它不是local的，对一个地方的修改会影响到全局。</li><li>对于空间域而言，在节点上定义卷积，对节点周围的邻居节点做平滑。但对于大多数节点而言，度很小，而小部分节点的度很大。</li></ul><h1 id="谱域的cnn方法">谱域的CNN方法</h1><p>一个图可以被定义为<span class="math inline">\(G =(V,E,W)\)</span>,其中<span class="math inline">\(n =|V|\)</span>为节点的个数，<span class="math inline">\(W \in R^{n \timesn}\)</span>为权重邻接矩阵。</p><p>每个节点有<span class="math inline">\(d\)</span>个feature，<spanclass="math inline">\(X \in R^{n \times d}\)</span>为节点的特征矩阵。</p><p>对于在非欧式距空间中对数据进行处理，我们可以考虑两种方法： 1.将图嵌入到欧式空间，再使用CNN对其进行处理，即embedding的方法 2.对CNN进行修改，使其能够用于图结构，即GCNN的方法</p><p>note：embedding嵌入什么空间取决于loss的设置。此外，若是需要在图中找pattern，则embedding的方式不适用，因为其找不到结构pattern。</p><h2 id="图上的傅里叶basis">图上的傅里叶basis</h2><p>图上的拉普拉斯L：对信号求导，越大越不平滑，是刻画图上平滑程度的算子。</p><p><span class="math display">\[L=D-W\]</span> D为对角阵，<spanclass="math inline">\(D_{ii} = \sum_j W_{ij}\)</span></p><p>L为非负的，对应信号的平滑程度，当值等于0时信号平滑。</p><p>Normalized graph Laplacien:</p><p><span class="math display">\[L = I - D^{- \frac{1}{2}} W D^{-\frac{1}{2}}\]</span> <span class="math inline">\(I\)</span>:Identitymatrix</p><p>在傅里叶变换中，利用基组使空间域映射到频域，即利用正弦波对信号进行分解。</p><p>图拉普拉斯可以被对角化为：</p><p><span class="math display">\[L = U \Lambda U^T\]</span></p><p><span class="math inline">\(\{u_l\}^{n-1}_{l=0}\)</span> :正交特征向量的完全集 <spanclass="math inline">\(\{\lambda_l\}^{n-1}_{l=0}\)</span> :相应的非负特征值</p><p>需要注意的是，都是正交才能进行点积。</p><p>其中<span class="math inline">\(U=[u_0,...,u_{n-1}],\Lambda =diag([\lambda_0,...,\lambda_{n-1}])\)</span></p><h2 id="图傅里叶变换">图傅里叶变换</h2><p>对于一个信号<span class="math inline">\(x \inR^n\)</span>,傅里叶变换定义为：</p><p><span class="math display">\[\hat{x} = U^T x\]</span></p><p>傅里叶逆变换：</p><p><span class="math display">\[ x =  U \hat{x} \]</span></p><p>傅里叶变换中的定理:卷积在进行傅里叶变换之后会变为点积</p><p>因此，x与y的卷积可以表示为：</p><p><span class="math display">\[x \ast_G y = U (( U^T x) \odot ( U^T y))\]</span></p><p>卷积的过程可以被拆解成三步：</p><ol type="1"><li>对x做傅里叶变换: <span class="math inline">\(U^T x\)</span></li><li>乘以卷积核:<span class="math inline">\(g_{\theta} U^Tx\)</span></li><li>做傅里叶逆变换: <span class="math inline">\(U g_{\theta} U^Tx\)</span></li></ol><p>因此卷积可以被写为：</p><p><span class="math display">\[x \*_G y = U g_{\theta} U^T x\]</span></p><p><span class="math display">\[x_{k+1,j} = h(\sum^{f_k}_{i=1} UF_{k,i,j} U^T x_{k,i})\]</span> <spanclass="math inline">\(x_{k,i}\)</span>:k+h 信号</p><p>缺点： 1. 要进行特征分解 2. 需要大量计算。U的复杂度为<spanclass="math inline">\(O(N^2)\)</span> 3. 不是localized</p><h2 id="chebynet">ChebyNet</h2><p>思想：参数化滤波器 <span class="math display">\[ g_{\theta} =diag([\theta_0,...,\theta_{n-1}]) \Rightarrow g_\beta(\Lambda) =\sum^{K-1}_{k = 0} \beta_k \Lambda^k \]</span></p><p>参数由n个减少为k个(<span class="math inline">\(\beta_k\)</span>)</p><p>ChebyNet: <span class="math display">\[ x *\ast_G y = U g_\beta(\Lambda) U^T x = \sum_{k=0}^{K-1} \beta_k L^k x \]</span></p><p>U被约了，只留下了L，而L是稀疏的。通过约束空间，来使使其变成local。</p><p>优点： 1. 无需特征分解 2. 卷积核是local的 3. 计算复杂度：<spanclass="math inline">\(O(n^2) \rightarrow O(|E|)\)</span></p><h2 id="graph-wavelet-neural-networkgwnn">Graph wavelet neuralnetwork(GWNN)</h2><p>ChebyNet通过特征值矩阵<spanclass="math inline">\(\Lambda\)</span>的多项式方程来限制图滤波器的空间来达到localized卷积：</p><p><span class="math display">\[ g_\theta(\Lambda) = \sum^{K-1}_{k=0}\theta_k \Lambda^k \]</span></p><p>GWNN通过使用小波基对基进行替换来达到localized图卷积的效果。</p><p>傅里叶basis <span class="math inline">\(U\)</span>： - 稠密 - 非局部- 高计算成本</p><p>小波basis <span class="math inline">\(\psi_s = U e^{\lambda_s}U^T\)</span>： - 稀疏 - 局部 - 低计算成本</p><p>主要思想：从特征变换中剥离图卷积</p><p><span class="math display">\[x_{k+1,j} = h(\sum^p_{i=1} \psi_sF_{k,i,j} \psi^{-1}_s x_{k,i}), \quad j = 1,...,q \]</span></p><p>特征变换部分：</p><p><span class="math display">\[y_{k,j} = \sum^{p}_{i=1} T_{ji}x_{k,i}\]</span></p><p>图卷积部分：</p><p><span class="math display">\[x_{k+1,j} = h(\psi_s F_k \psi^{-1}_sy_{k,j})\]</span></p><h1 id="空间域cnn方法">空间域CNN方法</h1><h2 id="类比方法">类比方法</h2><p>将卷积过程拆分成三步： 1. 确定邻居 2. 确定邻居的顺序 3.参数共享（卷积核里，在定序的基础上）</p><p>问题：邻居的数量不同（有的多有的少），然而卷积核的大小是固定的。<br />解决方式：给定中心节点，确定其到其他节点的<strong>距离函数</strong>，选前TOPK个邻居。</p><h2 id="graphsage">GraphSAGE</h2><p>分为两步： 1. 以某个节点为中心对其邻居节点进行采样 2.做aggregate（带权平均）</p><p><span class="math display">\[a_v^{(k)} =AGGREGATE^{(k)}(\{h_u^{(k-1)} : U \in N(v) \}) \]</span> <spanclass="math display">\[h_v^(k) = COMBINE^{(k)} (h_v^{(k-1)},a_v^{(k)})\]</span></p><h2 id="gcn">GCN</h2><p><span class="math display">\[Z = f(X,A) = softmax(\hat{A}ReLU(\hat{A}XW^{(0)})) W^{(1)})\]</span></p><p><span class="math inline">\(\hat{A}\)</span> :邻接矩阵的变形，给定，与结构有关<br /><span class="math inline">\(W^{(0)}, W^{(1)}\)</span>:特征变换参数<br /><span class="math inline">\(X\)</span> : feature</p><p>对<span class="math inline">\(\hat{A}\)</span>进行参数化 <spanclass="math inline">\(\rightarrow\)</span> 加attention机制：</p><p><span class="math display">\[\alpha_{ij} = \frac{exp(Leaky \quadReLU(\overset{\sim}a [W \vec{h}_i ||W \vec{h}_j]))}{\sum_{k \in N_i}exp(Leaky \quad ReLU(\overset{\sim}a [W \vec{h}_i ||W\vec{h}_j]))}\]</span></p><p>本质上是求节点i和j的相似度。</p><h2 id="monet">MoNet</h2><p>是所有空间方法的通用形式。</p><p><span class="math display">\[(f*g)(x) = \sum_{j=1}^J g_j D_j(x)f\]</span></p><p><span class="math inline">\(D_j (x)\)</span> : kernelfunction,在频域中为基<br /><spanclass="math inline">\(g_j\)</span>:kernel,可以定义为相似度函数。整个公式看做是相似度函数的加权</p><p>谱域：显式定义变换 空间域：不显式定义变换</p><h1 id="graph-pooling">Graph pooling</h1><ol type="1"><li>节点粗化</li><li>节点选择</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;CNN无法处理非欧式数据，又希望在拓扑图上有效地提取空间特征来进行机器学习。因此GCNN成为了研究重点。&lt;/p&gt;</summary>
    
    
    
    <category term="graph" scheme="http://yoursite.com/categories/graph/"/>
    
    
    <category term="Graph" scheme="http://yoursite.com/tags/Graph/"/>
    
    <category term="GCN" scheme="http://yoursite.com/tags/GCN/"/>
    
  </entry>
  
  <entry>
    <title>A Survey on Few-shot Learning</title>
    <link href="http://yoursite.com/2020/06/12/Overview-of-Few-shot-Learning/"/>
    <id>http://yoursite.com/2020/06/12/Overview-of-Few-shot-Learning/</id>
    <published>2020-06-12T11:44:35.000Z</published>
    <updated>2023-03-24T09:21:55.306Z</updated>
    
    <content type="html"><![CDATA[<p>最近开始研究利用少量样本进行机器学习。<ahref="https://arxiv.org/abs/1904.05046v2">Wang et al.</a>系统性地对小样本学习的工作做了一个总结，本篇对该论文以及相关的工作进行了一个梳理。</p><span id="more"></span><h1 id="introduction">INTRODUCTION</h1><p><strong>Main idea:</strong> Use <strong>prior knowledge</strong> toalleviate the problem of having an unreliable empirical risk minimizerin FSL supervised learning.</p><p><a href="https://imgchr.com/i/tOlmqS"><imgsrc="https://s1.ax1x.com/2020/06/12/tOlmqS.md.png"alt="tOlmqS.md.png" /></a></p><h1 id="methods">METHODS</h1><p>小样本学习的方法主要可以被归为三个方面：数据，模型，以及算法。其核心都是如何更好地利用先验知识。</p><p>从数据入手，主要是利用先验知识产生更大数量的数据；从模型入手，主要思想是利用先验知识来限制假设空间的大小，从而更快的找到最优点；从算法入手，主要思想是利用先验知识来改变搜索策略。</p><h2 id="data">Data</h2><p><a href="https://imgchr.com/i/tO1uy6"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1uy6.md.png"alt="tO1uy6.md.png" /></a></p><p>通过数据增强小样本的方法不改变搜索空间的大小，而是通过产生数据来帮助找到最优参数。它可以被归类为以下三种方法：<a href="https://imgchr.com/i/tO1ZWR"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1ZWR.md.png"alt="tO1ZWR.md.png" /></a></p><ol type="1"><li>Transforming Samples from <spanclass="math inline">\(D_{train}\)</span><br />该类方法通过对原训练数据的操作（如图像领域中的翻转等）进行数据增强，从而达到扩充数据的效果。</li><li>Transforming Samples from a Weakly Labeled or Unlabeled DataSet<br />该类方法利用从训练数据中学习到的predictor对无监督或者弱监督的数据集进行标注，从而获得更大规模的数据集。</li><li>Transforming Samples from Similar Data Sets<br />该类方法从一个相似且更大规模的数据集上迁移input-outputpairs，aggregation weight通常基于样本间的相似性。</li></ol><h2 id="model">Model</h2><p>Main idea: Use prior knowledge to constrain the complexity of ℋ,which results in a much smaller hypothesis space <spanclass="math inline">\(\widetilde{H}\)</span>.</p><p><a href="https://imgchr.com/i/tO1VY9"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1VY9.md.png"alt="tO1VY9.md.png" /></a></p><h3 id="multitask-learning">Multitask Learning</h3><p><strong>Multitask learning</strong> learns multiple related taskssimultaneously by exploiting both task-generic and task-specificinformation.</p><p>Given <span class="math inline">\(C\)</span> related tasks <spanclass="math inline">\(T_1,…,T_C\)</span>, each <spanclass="math inline">\(T_c\)</span> has a dataset<spanclass="math inline">\(D_c=\{D_{train}^c, D_{test}^c\}\)</span>.</p><p>The few-shot tasks are regarded as target tasks, and the rest assource tasks. Multitask learning learns from <spanclass="math inline">\(D_{train}^c\)</span> to obtain <spanclass="math inline">\(\theta_c\)</span> for train each <spanclass="math inline">\(T_c\)</span>.</p><h4 id="parameter-sharing">Parameter Sharing</h4><p><strong>Parameter Sharing</strong> directly shares some parametersamong tasks.</p><p><a href="https://imgchr.com/i/tO1nQx"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1nQx.md.png"alt="tO1nQx.md.png" /></a></p><h4 id="parameter-tying">Parameter Tying</h4><p><strong>Parameter Tying</strong> encourages parameters of differenttasks to be similar. A popular approach is by regularizing the <spanclass="math inline">\(\theta_c\)</span>.</p><p><a href="https://imgchr.com/i/tO1mS1"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1mS1.md.png"alt="tO1mS1.md.png" /></a></p><h3 id="embedding-learning">Embedding Learning</h3><p><strong>Embedding learning</strong> embeds each sample <spanclass="math inline">\(x_i \in \mathcal{X} \subseteq R^d\)</span> to alower-dimensional <span class="math inline">\(z_i \in \mathcal{Z}\subseteq R^m\)</span>.</p><p>It have three key components: - A function <spanclass="math inline">\(f\)</span> which embeds test sample <spanclass="math inline">\(x_{test} \in D_test\)</span> to <spanclass="math inline">\(\mathcal{Z}\)</span> - A function <spanclass="math inline">\(g\)</span> which embeds training sample <spanclass="math inline">\(x_{train} \in D_{train}\)</span> to <spanclass="math inline">\(\mathcal{Z}\)</span> - A similarity function <spanclass="math inline">\(s(·,·)\)</span> which measures the similaritybetween <span class="math inline">\(f(x_{test})\)</span> and <spanclass="math inline">\(g(x_I)\)</span> in <spanclass="math inline">\(\mathcal{Z}\)</span></p><p><a href="https://imgchr.com/i/tO1QeO"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1QeO.md.png"alt="tO1QeO.md.png" /></a></p><h4 id="task-specific-embedding-model">Task-specific embeddingmodel</h4><p>Learn an embedding function tailored for each task, by using onlyinformation from that task.</p><h4 id="task-invariant-i.e.-general-embedding-model">Task-invariant(i.e., general) embedding model</h4><p>Learn a general embedding function from a large-scale data setcontaining sufficient samples with various outputs, and then directlyuse this on the new few-shot <spanclass="math inline">\(D_{train}\)</span> without retraining.</p><p>E.g. Meta-learning (Matching Nets, Prototypical Networks, etc.),Siamese Net, etc.</p><p><a href="https://imgchr.com/i/tO1KOK"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1KOK.md.png"alt="tO1KOK.md.png" /></a></p><h4id="hybrid-embedding-model-encodes-both-task-specific-and-task-invariant-information">Hybridembedding model (encodes both task-specific and task-invariantinformation)</h4><p>Learn a function which takes information extracted from <spanclass="math inline">\(D_{train}\)</span> as input and returns anembedding which acts as the parameter for <spanclass="math inline">\(f(·)\)</span> .</p><p><a href="https://imgchr.com/i/tO11Te"><imgsrc="https://s1.ax1x.com/2020/06/12/tO11Te.md.png"alt="tO11Te.md.png" /></a></p><h3 id="learning-with-external-memory">Learning with ExternalMemory</h3><p><strong>Learning with external memory</strong> extracts knowledgefrom <span class="math inline">\(D_{train}\)</span>, and stores it in anexternal memory. Each new sample <spanclass="math inline">\(D_{test}\)</span> is then represented by aweighted average of contents extracted from the memory.</p><p>A <strong>key-value memory</strong> is usually used in FSL.<br />Let the memory be <span class="math inline">\(M \in R^{(b \timesm)}\)</span>, with each of its <span class="math inline">\(b\)</span>memory slots <span class="math inline">\(M(i) \in R^m\)</span>consisting of a key-value pair <spanclass="math inline">\(M(i)=(M_{key}(i),M_{value}(i))\)</span>.<br />A test sample <span class="math inline">\(x_{test}\)</span> is firstembedded by an embedding function <spanclass="math inline">\(f\)</span>, and used to query for the similarmemory slots.</p><p>According to the functionality of the memory, it can be subdividedinto two types. 1. Refining Representations<br />Put <span class="math inline">\(x_{train}\)</span> into the memory, suchthat the stored key-value pairs can represent <spanclass="math inline">\(x_{test}\)</span> more accurately. 2. RefiningParameters<br />Use a memory to parameterize the embedding function <spanclass="math inline">\(g(·)\)</span> for a new <spanclass="math inline">\(x_{test}\)</span> or classification model.</p><p><a href="https://imgchr.com/i/tO1lwD"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1lwD.md.png"alt="tO1lwD.md.png" /></a></p><h3 id="generative-modeling">Generative Modeling</h3><p><strong>Generative modeling methods</strong> estimate the probabilitydistribution <span class="math inline">\(p(x)\)</span> from the observed<span class="math inline">\(x_i\)</span> with the help of priorknowledge.</p><p>The observed <span class="math inline">\(x\)</span> is assumed to bedrawn from some distribution <spanclass="math inline">\(p(x;\theta)\)</span> parameterized by <spanclass="math inline">\(\theta\)</span>.<br />Usually, there exists a latent variable <span class="math inline">\(z\backsim p(z;\gamma)\)</span>, so that <span class="math inline">\(x\backsim \int p(x|z;\theta)p(z;\gamma)dz\)</span>.<br />The prior distribution <span class="math inline">\(p(z;\gamma)\)</span>brings in prior knowledge.</p><p>According to what the latent variable 𝑧 represents, it can be groupedinto three types. 1. Decomposable Components<br />Samples may share some smaller decomposable components with samples fromthe other tasks. One then only needs to find the correct combination ofthese decomposable components, and decides which target class thiscombination belongs to. 2. Groupwise Shared Prior<br />Similar tasks have similar prior probabilities, and this can beutilized. 3. Parameters of Inference Networks<br />To find the best <span class="math inline">\(\theta\)</span>, one has tomaximize the posterior</p><p><span class="math display">\[p(z|x;\theta,\gamma)=\frac{p(x,z;\theta,\gamma)}{p(x;\gamma)}=\frac{p(x|z;\theta)p(z;\gamma)}{\intp(x|z;\theta)p(z;\gamma)dz}\]</span></p><p><a href="https://imgchr.com/i/tO1tSI"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1tSI.md.png"alt="tO1tSI.md.png" /></a></p><h3 id="summary">Summary</h3><ol type="1"><li><p>Multitask learning<br /><strong>Scenarios:</strong> Existing similar tasks or auxiliarytasks.<br /><strong>Limitations:</strong> Joint training of all the tasks togetheris required. Thus, when a new few-shot task arrives, the whole multitaskmodel has to be trained again.</p></li><li><p>Embedding learning<br /><strong>Scenarios:</strong> A large-scale data set containing sufficientsamples of various classes is available.<br /><strong>Limitations:</strong> May not work well when the few-shot taskis not closely related to the other tasks.</p></li><li><p>Learning with External Memory<br /><strong>Scenarios:</strong> A memory network is available.<br /><strong>Limitations:</strong> Incurs additional space and computationalcosts, which increase with memory size.</p></li><li><p>Generative modeling<br /><strong>Scenarios:</strong> Performing tasks such as generation andreconstruction.<br /><strong>Limitations:</strong> High inference cost, and more difficult toderive than deterministic models.</p></li></ol><h2 id="algorithm">ALGORITHM</h2><p>The algorithm is the strategy to search in the hypothesis space <spanclass="math inline">\(H\)</span> for the parameter <spanclass="math inline">\(\theta\)</span> of the best hypothesis <spanclass="math inline">\(h^*\)</span>.</p><p>Methods in this section use prior knowledge to influence how <spanclass="math inline">\(\theta\)</span> is obtained, either by: 1.providing a good initialized parameter <spanclass="math inline">\(\theta_0\)</span> 2. directly learning anoptimizer to output search steps</p><p>In terms of how the search strategy is affected by prior knowledge,the methods can be classified into three groups.</p><p><a href="https://imgchr.com/i/tO5Z4S"><imgsrc="https://s1.ax1x.com/2020/06/12/tO5Z4S.md.png"alt="tO5Z4S.md.png" /></a></p><h3 id="refining-existing-parameters">Refining Existing Parameters</h3><p><strong>Refining existing parameters:</strong> An initial <spanclass="math inline">\(\theta_0\)</span> learned from other tasks, and isthen refined using <span class="math inline">\(D_{train}\)</span>.<br />The assumption is that <span class="math inline">\(\theta_0\)</span>captures some general structures of the large-scale data. Therefore, itcan be adapted to <span class="math inline">\(D\)</span> with a fewiterations.</p><p>Given the few-shot <span class="math inline">\(D_{train}\)</span> ,simply fine-tuning <span class="math inline">\(\theta_0\)</span> bygradient descent may lead to overfitting. Hence, methods fine-tune <spanclass="math inline">\(\theta_0\)</span> by regularization to preventoverfitting. They can be grouped as follows:</p><ol type="1"><li>Early-stopping .<br />It requires separating a validation set from D train to monitor thetraining procedure. Learning is stopped when there is no performanceimprovement on the validation set.</li><li>Selectively updating <span class="math inline">\(\theta_0\)</span>.<br />Only a portion of <span class="math inline">\(\theta_0\)</span> isupdated in order to avoid overfitting.</li><li>Updating related parts of <spanclass="math inline">\(\theta_0\)</span> together.<br />One can group elements of <span class="math inline">\(\theta_0\)</span>(such as the neurons in a deep neural network), and update each groupjointly with the same update information.</li><li>Using a model regression network.<br />A model regression network captures the task-agnostic transformationwhich maps the parameter values obtained by training on a few examplesto the parameter values that will be obtained by training on a lot ofsamples.</li></ol><h4 id="aggregating-a-set-of-parameters">Aggregating a Set ofParameters</h4><p>Sometimes, we do not have a suitable <spanclass="math inline">\(\theta_0\)</span> to start with. Instead, we havemany models that are learned from related tasks.</p><p><a href="https://imgchr.com/i/tO100S"><imgsrc="https://s1.ax1x.com/2020/06/12/tO100S.md.png"alt="tO100S.md.png" /></a></p><p>Instead of using the augmented data directly, the following methodsuse models (with parameters <spanclass="math inline">\(\theta_0\)</span>’s) pre-trained from these datasets. The problem is then how to adapt them efficiently to the new taskusing <span class="math inline">\(D_{train}\)</span> .</p><ol type="1"><li>Unlabeled data set.<br />One can pre-train functions from the unlabeled data to cluster andseparate samples well. A neural network is then used to adapt them tothe new task with the few-shot <spanclass="math inline">\(D_{train}\)</span>.</li><li>Similar data sets.<br />few-shot object classification can be performed by leveraging samplesand classifiers from similar classes. First, it replaces the features ofsamples from these similar classes by features from the new class. Thelearned classifier is then reused, and only the classification thresholdis adjusted for the new class.</li></ol><h4 id="fine-tuning-existing-parameter-with-new-parameters">Fine-TuningExisting Parameter with New Parameters</h4><p>The pre-trained <span class="math inline">\(\theta_0\)</span> may notbe enough to encode the new FSL task completely. Hence, an additionalparameter(s) <span class="math inline">\(\delta\)</span> is used to takethe specialty of D train into account. Specifically, this strategyexpands the model parameter to become <span class="math inline">\(\theta= \{\theta_0,\delta\}\)</span>, and fine-tunes <spanclass="math inline">\(\theta_0\)</span> while learning <spanclass="math inline">\(\delta\)</span>.</p><p><a href="https://imgchr.com/i/tO1Nlt"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1Nlt.md.png"alt="tO1Nlt.md.png" /></a></p><h3 id="refining-meta-learned-parameters">Refining meta-learnedparameters</h3><p>Methods in this section use meta-learning to refine the meta-learnedparameter <span class="math inline">\(\theta_0\)</span>.The <spanclass="math inline">\(\theta_0\)</span> is <strong>continuouslyoptimized</strong> by the meta-learner according to performance of thelearner.</p><p><a href="https://imgchr.com/i/tO1aOf"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1aOf.md.png"alt="tO1aOf.md.png" /></a></p><p>The meta-learned <span class="math inline">\(\theta_0\)</span> isoften refined by gradient descent. A representative method is the <ahref="https://arxiv.org/abs/1703.03400">ModelAgnostic Meta-Learning(MAML)</a>.</p><p><a href="https://imgchr.com/i/tX3dSA"><imgsrc="https://s1.ax1x.com/2020/06/12/tX3dSA.md.png"alt="tX3dSA.md.png" /></a></p><p>Recently, many improvements have been proposed for MAML, mainly alongthe following three aspects:</p><ol type="1"><li>Incorporating task-specific information.<br />MAML provides the same initialization for all tasks. However, thisneglects task-specific information, and is appropriate only when the setof tasks are all very similar.</li><li>Modeling the uncertainty of using a meta-learned <spanclass="math inline">\(\theta_0\)</span>.<br />The ability to measure this uncertainty provides hints for activelearning and further data collection . There are works that consideruncertainty for the meta-learned <spanclass="math inline">\(\theta_0\)</span>, uncertainty for thetask-specific <span class="math inline">\(\phi_s\)</span>, anduncertainty for class n’s class-specific parameter<spanclass="math inline">\(\phi_{s,n}\)</span>.</li><li>Improving the refining procedure.<br />Refinement by a few gradient descent steps may not be reliable.Regularization can be used to correct the descent direction.</li></ol><h3 id="learning-the-optimizer">Learning the optimizer</h3><p>Instead of using gradient descent, methods in this section learns anoptimizer which can directly output the update (<spanclass="math inline">\(\sum_{i=1}^t\Delta\theta^{i-1}\)</span>). There isthen no need to tune the stepsize <spanclass="math inline">\(\alpha\)</span> or find the search direction, asthe learning algorithm does that automatically.</p><p><a href="https://imgchr.com/i/tO1wm8"><imgsrc="https://s1.ax1x.com/2020/06/12/tO1wm8.md.png"alt="tO1wm8.md.png" /></a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近开始研究利用少量样本进行机器学习。&lt;a
href=&quot;https://arxiv.org/abs/1904.05046v2&quot;&gt;Wang et al.&lt;/a&gt;
系统性地对小样本学习的工作做了一个总结，本篇对该论文以及相关的工作进行了一个梳理。&lt;/p&gt;</summary>
    
    
    
    <category term="survey" scheme="http://yoursite.com/categories/survey/"/>
    
    
    <category term="Few-shot Learning" scheme="http://yoursite.com/tags/Few-shot-Learning/"/>
    
    <category term="Transfer Learning" scheme="http://yoursite.com/tags/Transfer-Learning/"/>
    
    <category term="Meta Learning" scheme="http://yoursite.com/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Multi-Task Identiﬁcation of Entities, Relations, and Coreference for Scientiﬁc Knowledge Graph Construction</title>
    <link href="http://yoursite.com/2019/09/24/Multi-Task-Identi%EF%AC%81cation-of-Entities-Relations-and-Coreference-for-Scienti%EF%AC%81c-Knowledge-Graph-Construction/"/>
    <id>http://yoursite.com/2019/09/24/Multi-Task-Identi%EF%AC%81cation-of-Entities-Relations-and-Coreference-for-Scienti%EF%AC%81c-Knowledge-Graph-Construction/</id>
    <published>2019-09-24T09:43:37.000Z</published>
    <updated>2023-03-24T09:21:50.100Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了一种基于共享span表示进行多任务分类的框架（实体分类、关系分类、共指消解），将科学文献摘要抽取成知识图。</p><p>Accepted by: EMNLP 2018 Paperlink：http://ssli.ee.washington.edu/~luanyi/YiLuan_files/EMNLP2018_YL_sciIE.pdf</p><span id="more"></span><h1 id="challenge">Challenge</h1><ul><li>尽管搜索引擎取得了进步，但仍然很难确定新技术及其与之前技术存在的关系</li><li>科学文本的注释需要领域专业知识，这使得注释成本高昂并限制了资源</li><li>大多数关系提取系统都是针对句内关系而设计的，但是从科学文章中提取信息需要提取句子之间的关系</li></ul><h1 id="contribution">Contribution</h1><ul><li>开发了一个统一的学习模型（科学信息提取器框架，SCIIE），用于提取科学实体、关系和共指消解</li><li>创建了SCIERC数据库，包含科学名词、关系类别以及共指链接的标注（500个标注的科学摘要）<ul><li>6种实体</li><li>7种关系</li><li>共指链接在实体间的标注</li></ul></li></ul><h1 id="dataset">Dataset</h1><p>该论文提出了SCIERC数据集，包括了500个科学摘要，标记了其中的实体类型、实体间的关系以及共指关系。</p><p>下载链接：http://nlp.cs.washington.edu/sciIE/</p><figure><imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3VrdmRKQS5wbmc?x-oss-process=image/format,png#pic_center"alt="annotation" /><figcaption aria-hidden="true">annotation</figcaption></figure><center>Fig 1.标注实例.</center><h1 id="model">Model</h1><h2 id="主要思想">主要思想</h2><p>在低级别任务间共享参数，将三个任务看成共享span表示的多项分类问题</p><h2 id="问题定义">问题定义</h2><p>输入：单词序列 <span class="math inline">\(D = {w_1,...,w_n}\)</span>;</p><p>所有可能的句内词序列span <span class="math inline">\(S = {s_1, ... ,s_N}\)</span></p><p>输出：所有Span(即S)的实体类型E;</p><p><span class="math inline">\(S \times S\)</span>的关系R;</p><p>S中所有span的共指关系C</p><h2 id="模型定义">模型定义</h2><p>将多任务学习定义为条件概率分布: <spanclass="math inline">\(P(E,R,C|D)\)</span></p><p>为了高效地训练和推理，将该分部拆分成： <spanclass="math inline">\(P(E,R,C|D) = P(E,R,C,S|D) = \prod^N_{i=1} P(e_i|D) P(c_i | D)\prod^N_{j=1}P(r_{ij}|D)\)</span></p>其中每个随机变量的条件概率是独立标准化的： <spanclass="math display">\[P(e_i= e|D) = \frac{exp(\Phi_E (e,s_i))}{\sum_{e&#39; \in L_E} exp(\Phi_E(e&#39;,s_i))}\]</span> <spanclass="math display">\[P(r_{ij}= r|D) = \frac{exp(\Phi_R (r,s_i,s_j))}{\sum_{r&#39; \in L_R} exp(\Phi_R(r&#39;,s_i,s_j))}\]</span> <spanclass="math display">\[P(c_i= j|D) = \frac{exp(\Phi_C (s_i,s_j))}{\sum_{j&#39; \in \{1,...,i-1,\epsilon \}}exp(\Phi_E(e&#39;,s_i))}\]</span> <imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3VrdmFpZC5wbmc?x-oss-process=image/format,png#pic_center"alt="Overview" /><center>Fig 2. 多任务设置，三个任务被看做是顶层共享Span表示的分类问题.</center><h2 id="目标函数">目标函数</h2><p>给定一组所有文档D，模型损失函数被定义为所有三个任务的负对数似然丢失的加权和：</p><p><span class="math display">\[-\sum_{(D,R^*,E^*,C^*) \in D}\{\lambda_E logP(E^* |D) + \lambda_R logP(R^*|D) + \lambda_C logP(C^*|D)\}\]</span></p><p><span class="math inline">\(R^*, E^*, C^*\)</span> 为goal。 令<spanclass="math inline">\(C_i^*\)</span> 为span i里所有正确的祖先。即：<span class="math display">\[log P(C^*|D = \sum_{i=1...N}log\sum_{c \inC^*_i} P(c|D))\]</span> ## 打分结构 -在共享跨度表示g上使用前馈神经网络（FFNN）来计算一组跨度和成对跨度的分数。</p><pre><code>- $\phi_e(s_i)$:一个跨度$s_i$有一个实体类型e的可能性；      - $\phi_&#123;mr&#125;(s_i)$跨度$s_i$在一个关系r中被提到的可能性；       - $\phi_&#123;mc&#125;(s_i)$跨度$s_i$在一条共指链接中被提到的可能性- $\phi_r(s_i,s_j)$两个跨度与一个关系相结合的可能性；- $\phi_c(s_i,s_j)$两个跨度与一条共指链接相结合的可能性- $g_i$：$s_i$的向量表示 (ELMo)：连接来自BiLSTM的输出的$s_i$的左端点和右端点，基于注意力的软词条，嵌入span的宽度特征- span打分以及成对的span打分的计算方式：$$\phi_x(s_i) = w_x \dot FFNN_x(g_i)\phi_y(s_i,s_j)=w_y \dot FFNN_y([g_i,g_j,g_i \odot g_j])$$</code></pre><ul><li>这些分数被用于计算最终打分: <span class="math display">\[\Phi_E(e,s_i) = \phi_e(s_i)\]</span> <span class="math display">\[\Phi_R(r,s_i,s_j) = \phi_{mr}(s_i) + \phi_{mr}(s_j) + \phi_r(s_i,s_j)\]</span><span class="math display">\[\Phi_C (s_i,s_j) = \phi_{mc}(s_i) +\phi_{mc}(s_j) + \phi_c(s_i,s_j)\]</span></li></ul><h2 id="推理和剪枝">推理和剪枝</h2><p>使用波束搜索，根据打分排序减少训练和测试中成对的span的数量以将计算复杂度降低到<spanclass="math inline">\(O(n)\)</span>.</p><p>使用<spanclass="math inline">\(B_C\)</span>修剪共指消解任务中的span，使用<spanclass="math inline">\(B_R\)</span>修剪关系抽取任务中的span。</p><p>使用span score <span class="math inline">\(\phi_{mc}\)</span>和 <spanclass="math inline">\(\phi_{mr}\)</span>对波束中的span进行排序，波束的大小用<spanclass="math inline">\(\lambda_Cn\)</span>和<spanclass="math inline">\(\lambda_Rn\)</span>进行约束。span的最大宽度为W。</p><h1 id="kg-construction">KG Construction</h1><p>为了构建整个语料库中的知识图，先将SCIIE用于单个文档（摘要），再将多个文档中的实体和关系集成。</p><ul><li>结点（实体）抽取<ul><li>句子被启发式地标准化<ul><li>利用共指链接，将不通用词汇替换为通用词汇</li><li>采用字符长度最长的实体名称（使用全名替代首字母缩略）</li><li>使用单数标准化所有复数</li></ul></li><li>计算整个语料库中的实体出现频率<ul><li>出现频率&gt;k的实体为它分配结点</li><li>将频繁出现的、为子字符串的剩余实体进行合并</li></ul></li></ul></li><li>边（关系）分配<ul><li>在整个语料库中计算一对实体间的关系出现频率，选择频率最大的关系</li></ul></li></ul><figure><imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3VrdnRkZS5wbmc?x-oss-process=image/format,png#pic_center"alt="知识图谱构建过程" /><figcaption aria-hidden="true">知识图谱构建过程</figcaption></figure><center>Fig 3. 知识图谱的构建过程.</center><figure><imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3VrdndSSS5wbmc?x-oss-process=image/format,png#pic_center"alt="自动构建的知识图" /><figcaption aria-hidden="true">自动构建的知识图</figcaption></figure><center>Fig 4. 自动构建的知识图.</center><h1 id="results">Results</h1><figure><imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3Vrdk5JSC5wbmc?x-oss-process=image/format,png#pic_center"alt="RESULT" /><figcaption aria-hidden="true">RESULT</figcaption></figure><center>Table 1. 三个任务上与其他系统的性能对比.</center><figure><imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3VrdkRRUC5wbmc?x-oss-process=image/format,png#pic_center"alt="ablation" /><figcaption aria-hidden="true">ablation</figcaption></figure><center>Table 2. 多任务学习的消融研究.</center><h1 id="kg-analysis">KG Analysis</h1><h2 id="科学趋势分析">科学趋势分析</h2><figure><imgsrc="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zMi5heDF4LmNvbS8yMDE5LzA5LzI0L3VrdjB6dC5wbmc?x-oss-process=image/format,png#pic_center"alt="科学趋势分析" /><figcaption aria-hidden="true">科学趋势分析</figcaption></figure><center>Fig 5. 关键词 神经网络在NLP.Y轴：在该任务中使用神经网络的论文与使用其他论文的比例.</center>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文提出了一种基于共享span表示进行多任务分类的框架（实体分类、关系分类、共指消解），将科学文献摘要抽取成知识图。&lt;/p&gt;
&lt;p&gt;Accepted by: EMNLP 2018 Paper
link：http://ssli.ee.washington.edu/~luanyi/YiLuan_files/EMNLP2018_YL_sciIE.pdf&lt;/p&gt;</summary>
    
    
    
    <category term="paper-reading" scheme="http://yoursite.com/categories/paper-reading/"/>
    
    
    <category term="knowledge graph" scheme="http://yoursite.com/tags/knowledge-graph/"/>
    
    <category term="multi-task learning" scheme="http://yoursite.com/tags/multi-task-learning/"/>
    
    <category term="paper-reading" scheme="http://yoursite.com/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>Overview of Crawler Technology</title>
    <link href="http://yoursite.com/2019/03/22/Overview-of-Crawler-Technology/"/>
    <id>http://yoursite.com/2019/03/22/Overview-of-Crawler-Technology/</id>
    <published>2019-03-22T11:44:35.000Z</published>
    <updated>2023-03-24T09:21:53.496Z</updated>
    
    <content type="html"><![CDATA[<p>大致记录了一下爬虫的整体思想和一些常用的库方便日后项目中进行查阅。本文的主要内容整理自崔庆才的爬虫教程。<span id="more"></span> ## 爬虫基本流程 1. 发起请求通过HTTP库向目标站点发起请求，即发送一个request，请求可以包含额外的header等信息，等待服务器响应2. 获取响应内容如果服务器能正常响应，会得到一个response，response的内容便是所要获取的页面内容，类型可能有HTML,Json字符串，二进制数据（如图片视频）等类型。3. 解析内容得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理4. 保存数据保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件</p><h2 id="request-和-response">Request 和 Response</h2><h3 id="request">Request</h3><ol type="1"><li>请求方式主要有<strong>GET、POST</strong>两种类型，另外还有HEAD、PUT、DELETE、OPTIONS等GET和POST的区别： get：直接输入URL回车访问post：需要构建表单，点击表单提交，请求参数不会包含在URL后</li><li>请求URLURL全称统一资源定位符，如一个网页文档、一张图片、一个视频都可以用URL唯一来确定。&gt; 在渲染过程中，浏览器会根据图片的URL重新发送请求，渲染出图片</li><li>请求头 包含请求时的头部信息，如User-Agent、Host、Cookies等信息<br />用于服务器判断配置信息</li><li>请求体 请求时额外携带的数据，如表单提交时的表单数据</li></ol><h3 id="response">Response</h3><ol type="1"><li>响应状态 判断网页响应状态<br />e.g.状态码：<br />200 成功<br />301 跳转<br />404 找不到服务器<br />505 服务器错误</li><li>响应头 如服务类型、内容长度、服务器信息、设置Cookie等等。</li><li>响应体最主要的部分，包含了请求资源的内容，如网页HTML、图片二进制数据等。</li></ol><h2 id="能抓取的数据">能抓取的数据</h2><ol type="1"><li>网页文本 如HTML文档、Json格式文本等。</li><li>图片 获取到的是二进制文件，保存为图片格式</li><li>视频 同为二进制文件，保存为视频格式即可。</li><li>其他 只要是能获取到的都能抓取</li></ol><h2 id="解析方式">解析方式</h2><ol type="1"><li>直接处理</li><li>Json解析</li><li>正则表达式</li><li>BeautifulSoup</li><li>PyQuery</li><li>XPath</li></ol><h2 id="怎样保存数据">怎样保存数据</h2><ol type="1"><li>文本 纯文本、Json、Xml等</li><li>关系型数据库 如MySQL、Oracle、SQLServer等具有结构化表结构形式存储。</li><li>非关系型数据库 如MongoDB、Redis等Key-Value形式存储。</li><li>二进制文件 如图片、视频、音频等等直接保存成特定形式即可。</li></ol><h1 id="urllib库基本使用">Urllib库基本使用</h1><h2 id="什么是urllib">什么是Urllib</h2><p>Python内置的HTTP请求库<br />urllib.request 请求模块<br />urllib.error 异常处理模块<br />urllib.parse url解析模块(工具模块)<br />urllib.robotparser robots.txt解析模块</p><h2 id="用法讲解">用法讲解</h2><h3 id="request-1">Request</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, [timeout, ]*, cafile=<span class="literal">None</span>, capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://python.org&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将hello以post的形式传递，完成一个post的请求；若不加data则以get的形式发送</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(&#123;<span class="string">&#x27;word&#x27;</span>:<span class="string">&#x27;hello&#x27;</span>&#125;).encoding=<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>,data=data)</span><br><span class="line"><span class="built_in">print</span>(response.read())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在设置时间内没有得到响应的话会抛出异常；http://httpbin.org/get会返回请求时的一些参数，以json形式</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>,timeout=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将异常时间设为0.1秒，对异常进行捕获</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason,socket.timeout):\\将错误的原因进行判断</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="response-1">Response</h3><h4 id="响应类型">响应类型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.python.org&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br></pre></td></tr></table></figure><h4 id="状态码响应头">状态码、响应头</h4><p>是判断响应是否成功的重要标志 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.python.org&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.status)</span><br><span class="line"><span class="built_in">print</span>(response.getheaders())</span><br><span class="line"><span class="built_in">print</span>(response.getheaders(<span class="string">&#x27;Server&#x27;</span>))    <span class="comment">#获取一个特定的响应头(Server)</span></span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&#x27;http://www.python.org&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))    <span class="comment">#read是获得一个响应体的内容</span></span><br></pre></td></tr></table></figure>若要发送更为复杂的request <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加入新的headers</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(<span class="string">&#x27;http://www.python.org&#x27;</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure></p><h3 id="handler">Handler</h3><p>相当于辅助工具，帮助我们进行其他的操作 #### 设置代理设置代理可以切换本地的ip地址使不被封 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">proxy_handler = urllib.request.ProxyHandler(&#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>:<span class="string">&#x27;http://127.0.0.1:9743&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;https://127.0.0.1:9743&#x27;</span>,</span><br><span class="line">&#125;)</span><br><span class="line">opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read())</span><br></pre></td></tr></table></figure> #### Cookie用来维持登录状态 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar() <span class="comment">#cookie创建为对象</span></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie: <span class="comment">#对cookie遍历</span></span><br><span class="line">    <span class="built_in">print</span>(item.name+<span class="string">&quot;=&quot;</span>+item.value)</span><br></pre></td></tr></table></figure>可将Cookie保存成文本文件，若cookie没有失效的话可以从文件中读出cookie，请求时附加cookie信息来保持登录状态。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将cookie保存为txt文件</span></span><br><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&quot;cookie.txt&quot;</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>,ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure> 还有一种保存方式 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用文本文件的形式把cookie存储，然后读取出来，然后把cookie再次放到request里面，请求出了这个网页，这样请求的结果就是登陆后才能看到的网页内容</span></span><br><span class="line"><span class="keyword">import</span> http.cookiejar,urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">&#x27;cookie.txt&#x27;</span>,ignore_discard=<span class="literal">True</span>,ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">filename = <span class="string">&quot;cookie.txt&quot;</span></span><br></pre></td></tr></table></figure> ### 异常处理<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#请求一个不存在的网页</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://cuiqingcai.com/index.htm&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason) <span class="comment">#捕捉的是URL的异常</span></span><br></pre></td></tr></table></figure>具体可以捕捉哪些异常，see:https://docs.python.org/3/library/urllib.html<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#请求一个不存在的网页</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://cuiqingcai.com/index.htm&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason,e.code,e.headers,sep=<span class="string">&#x27;\n&#x27;</span>) </span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.reason) </span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Request Successfully&#x27;</span>)</span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">&#x27;http://cuiqingcai.com/index.htm&#x27;</span>,timeout = <span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(e.reason))</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason,socket.timeout):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;TIME OUT&#x27;</span>)</span><br></pre></td></tr></table></figure> ### URL解析 #### urlparse传入一个URL，然后将URL进行分割，分割成几个部分，然后将各个部分依次进行复制<br />所有URL都可以按标准的结构划分 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">&#x27;&#x27;</span>, allow_fragments=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(<span class="string">&#x27;http://www.baidu.com/index.html&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(result),result)</span><br></pre></td></tr></table></figure>若没有协议类型，则默认为https类型 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">result = urlparse(www.baidu.com/index.html<span class="string">&#x27;,scheme=&quot;https&quot;)</span></span><br><span class="line"><span class="string">print(result)</span></span><br></pre></td></tr></table></figure>若allow_fragments=False，则其内容将拼接到前面的内容中（成为path或query或params）#### urlunparse urlparse的反函数，将url进行拼接 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunparse</span><br><span class="line"></span><br><span class="line">data = [<span class="string">&#x27;http&#x27;</span>,<span class="string">&#x27;www.baidu.com&#x27;</span>,<span class="string">&#x27;index.html&#x27;</span>,<span class="string">&#x27;user&#x27;</span>,<span class="string">&#x27;a=6&#x27;</span>,<span class="string">&#x27;comment&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(urlunparse(data))</span><br></pre></td></tr></table></figure> ####urljoin用于拼接url，若前面的url和后面的url不同，后面的字段会覆盖前面的字段 ####urlencode 将字典对象转变为url请求参数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;germey&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>:<span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">&#x27;http://www.baidu.com?&#x27;</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure> ### robotparser用来解析robot.txt文件</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大致记录了一下爬虫的整体思想和一些常用的库方便日后项目中进行查阅。本文的主要内容整理自崔庆才的爬虫教程。</summary>
    
    
    
    <category term="crawler" scheme="http://yoursite.com/categories/crawler/"/>
    
    
    <category term="Crawler" scheme="http://yoursite.com/tags/Crawler/"/>
    
  </entry>
  
  <entry>
    <title>Overview of Knowledge Graph</title>
    <link href="http://yoursite.com/2019/03/22/Knowledge-Graph-Overview/"/>
    <id>http://yoursite.com/2019/03/22/Knowledge-Graph-Overview/</id>
    <published>2019-03-22T11:26:34.000Z</published>
    <updated>2023-03-24T09:17:35.204Z</updated>
    
    <content type="html"><![CDATA[<p>在计算机世界中，节点和边的符号通过“符号具化（symbolgrounding）”表征物理世界和认知世界中的对象，并作为不同个体对认知世界中信息和知识进行描述和交换的桥梁。这种使用统一形式描述的知识描述框架便于知识的分享与利用。</p><span id="more"></span><h2 id="知识图谱的类型">知识图谱的类型</h2><ul><li>语言知识图谱<br />主要是存储人类语言方面的知识，其中比较典型是英文词汇知识图谱WordNet，它由同义词集和描述同义词集之间的关系构成。</li><li>常识知识图谱<br />主要有 Cyc和 ConceptNet等。其中 Cyc 由大量实体和关系以及支持推理的常识规则构成；ConceptNet 由大量概念以及描述它们之间关系的常识构成。</li><li>语言认知知识图谱<br />中文知网词库HowNet是一种典型的语言认知知识图谱（语言认知知识与常识知识区别不大，因为语言是人类表达和交换信息的主要载体），HowNet致力于描述认知世界中人们对词语概念的理解，基于词语义原，揭示词语的更小语义单元的含义。</li><li>领域知识图谱<br />针对特定领域构建的知识图谱，专门为特定的领域服务， 例如：医学知识图谱SIDER(Side Effect Resource) ，电影知识图谱 IMDB (Internet MovieDatabase)，音乐知识图谱MusicBrainz等，这些知识图谱在各自的领域都有着广泛的应用。</li><li>百科知识图谱<br />主要以 Linked Open Data (LOD)项目支持的开放知识 图谱为核心，主要有Freebase、DBpedia、YAGO和Wikidata等，它们在信息检索、问答系统等任务中有着重要应用。</li></ul><h2 id="知识图谱的生命周期">知识图谱的生命周期</h2><h3 id="知识体系构建">知识体系构建</h3><p>指采用什么样的方式表达知识，其核心是构建一个本体对目标知识进行描述。</p><p><strong>输入</strong>：领域（医疗、金融...）、应用场景<br /><strong>输出</strong>：领域知识本体<br /><strong>关键技术</strong>：Ontology Engineering</p><p>作为语义网的应用，知识图谱的知识建模采用语义网的知识建模方式，分为概念、关系、概念关系三元组三个层次，并利用<strong>资源描述框架(RDF</strong>)进行描述。</p><p>RDF 的基本数据模型包括了三个对象类型：<br />- 资源<br />能够使用RDF表示的对象称之为资源，包括互联网上的实体、事件和 概念等。 -谓词 (Predicate)<br />主要描述资源本身的特征和资源之间的关系。每一个谓词可以定义元知识，例如，谓词的头尾部数据值的类型（如定义域和值域）、谓词与其他谓词的关系（如逆关系）。- 陈述 (Statements)<br />一条陈述包含三个部分，通常称之为RDF三元组&lt;主体(subject)，谓词(predicate)，宾语(object)&gt;。其中主体是被描述的资源，谓词可以表示主体的属性，也可以表示主体和宾语之间关系。当表示属性时，宾语就是属性值；当表示关系时，宾语也是一个资源。</p><h3 id="知识获取">知识获取</h3><p>知识获取目标是从海量的文本数据中通过信息抽取的方式获取知识，其方法根据所处理数据源的不同而不同。</p><p>知识图谱中数据的主要来源有： - 结构化数据<br />置信度高，规模小，缺乏个性化的属性信息 - 半结构化数据<br />置信度高，规模较大，个性化信息，形式多样，含有噪声 -非结构化文本数据<br />- 纯文本<br />置信度低，复杂多样，规模大</p><p><strong>输入</strong>：领域知识本体；海量数据：文本、垂直站点、百科<strong>输出</strong>：领域知识（实体集合，实体关系/属性）<strong>主要技术</strong>：信息抽取；文本挖掘</p><h4 id="实体识别">实体识别</h4><p>实体识别任务的目标是从文本中识别实体信息。早期有关实体识别的研究主要是针对命名实体的识别。在知识图谱领域，从文本中识别实体不仅仅局限于命名实体，还包括其他类别的实体，特别是领域实体。与实体识别相关的任务是实体抽取。</p><h4 id="实体消歧">实体消歧</h4><p>目标是消除指定实体的歧义。实体消歧对于知识图谱构建和应用有着重要的作用，也是建立语言表达和知识图谱联系的关键环节。从技术路线上划分，实体消歧任务可以分为<em>实体链接</em>和<em>实体聚类</em>两种类型。</p><h4 id="关系抽取">关系抽取</h4><p>目标是获取两个实体之间的语义关系。</p><p>语义关系可以是一元关系（例如实体的类型），也可以是二元关系（例如实体的属性）甚至是更高阶的关系。在现有知识图谱中，所处理的语义关系通常指的是一元关系和二元关系。</p><p>根据抽取目标的不同，关系抽取任务可以分为： -关系分类任务：判别一句话中两个指定实体之间的语义关系。 -属性抽取任务：在给定一个实体以及一个预定义关系的条件下，抽取另外一个实体。- 关系实例抽取任务：给定关系类型，抽取满足该关系的实例数据。</p><h4 id="事件抽取">事件抽取</h4><p>目标是从描述事件信息的文本中抽取出用户感兴趣的事件信息并以结构化的形式呈现出来。</p><p>事件是发生在某个特定的时间点或时间段、某个特定的地域范围内，由一个或者多个角色参与的，一个或者多个动作组成的事情或者状态的改变。</p><p>现有知识图谱大多以实体和实体之间的关系为核心，缺乏事件知识。事件知识能弥补现有以实体和实体关系为核心的知识图谱知识表达能力不足的问题，是构建知识图谱不可或缺的技术。事件结构本身的复杂性以及自然语言表达的歧义性和灵活性，对事件抽取提出了很大的挑战。</p><p>根据抽取方法的不同，已有的事件抽取方法可以分为 -基于模式匹配的事件抽取 - 基于机器学习的事件抽取</p><h3 id="知识融合">知识融合</h3><p>对不同来源、不同语言或不同结构的知识进行融合，从而对于已有知识图谱进行补充、更新和去重。</p><p><strong>输入</strong>：抽取出来的知识；现有知识库；知识本体<br /><strong>输出</strong>：统一知识库；知识置信度<strong>关键技术</strong>：Ontology Matching；Entity Linking</p><p>从融合的对象看： -知识体系的融合：两个或多个异构知识体系进行融合，即对相同的类别、属性、关系进行映射。 -实例的融合：对于两个不同知识图谱中的实例（实体实例、关系实例）进行融合，包括不同知识体系下的实例、不同语言的实例。</p><p>从融合的知识图谱类型看： -竖直方向的融合：融合（较）高层通用本体与（较）底层领域本体 或实例数据 -水平方向的融合：融合同层次的知识图谱，实现实例数据的互补。</p><h3 id="知识存储和查询">知识存储和查询</h3><p>因为目前知识图谱大多是基于图的数据结构，它的存储方式 主要有两种形式：- RDF 格式存储：以三元组的形式存储数据 - 图数据库 (Graph Database)</p><p><strong>输入</strong>：大规模知识库知识<strong>输出</strong>：知识库存储和查询服务<strong>主要技术</strong>：知识表示；知识查询语言；存储/检索引擎</p><h3 id="知识推理">知识推理</h3><p>由于处理数据的不完备性，知识图谱中肯定存在知识缺失现象（包括实体缺失、关系缺失）。我们也很难利用抽取或者融合的方法对于缺失的知识进行补齐。因此，需要采用推理的手段发现已有知识中隐含的知识。</p><p>目前知识推理的研究主要集中在针对知识图谱中缺失关系的补足，即挖掘两个实体之间隐含的语义关系。所采用的方法可以分为两种：-基于传统逻辑规则的方法进行推理：研究热点在于如何自动学习推理规则，以及如何解决推理过程中的规则冲突问题。-基于表示学习的推理：即采用学习的方式，将传统推理过程转化为基于分布式表示的语义向量相似度计算任务。这类方法优点是容错率高、可学习，缺点也显而易见，即不可解释，缺乏语义约束。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在计算机世界中，节点和边的符号通过“符号具化（symbol
grounding）”表征物理世界和认知世界中的对象，并作为不同个体对认知世界中信息和知识进行描述和交换的桥梁。这种使用统一形式描述的知识描述框架便于知识的分享与利用。&lt;/p&gt;</summary>
    
    
    
    <category term="knowledge graph" scheme="http://yoursite.com/categories/knowledge-graph/"/>
    
    
    <category term="Knowledge Graph" scheme="http://yoursite.com/tags/Knowledge-Graph/"/>
    
  </entry>
  
  <entry>
    <title>Algorithm Design and Analysis - Dynamic programming</title>
    <link href="http://yoursite.com/2018/10/18/Algorithm-Design-and-Analysis-Dynamic-programming/"/>
    <id>http://yoursite.com/2018/10/18/Algorithm-Design-and-Analysis-Dynamic-programming/</id>
    <published>2018-10-18T13:52:10.000Z</published>
    <updated>2023-03-24T09:17:04.883Z</updated>
    
    <content type="html"><![CDATA[<p>Main message: 1. 把子问题的求解想象成多步求解过程 2.子问题的最优解可以组合成原问题的最优解 3.Programming：tabular可以被用于避免子问题的重复计算</p><span id="more"></span><p>联动：<a  href ="http://imjiawen.com/2018/08/14/Dynamic-programming-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/#more">DynamicProgramming 动态规划算法</a></p><h1 id="dynamic-programming-vs.-divide-and-conquer">Dynamic programmingVS. Divide-and-conquer</h1><p>动态规划算法一般被用于解决优化问题。需满足以下几个特点：</p><ol type="1"><li>原问题可以被分解成更小的子问题</li><li>具有最优子结构性质，即最优解可以通过结合子问题的最优解得到。</li></ol><p>与通用的分治法框架不同，动态规划算法通常<strong>枚举</strong>所有可能的分解策略。子问题的重复计算可以通过“programming”避免。</p><h1 id="matrix-chain-multiplication-problem-矩阵的链式乘法">Matrix ChainMultiplication problem 矩阵的链式乘法</h1><hr /><p>INPUT:A sequence of n matrices <span class="math inline">\(A_1, A_2 ,..., A_n\)</span>; matrix A i has dimension <spanclass="math inline">\(p_{i-1} \times p_i\)</span></p><p>OUTPUT:Fully parenthesizing the product <spanclass="math inline">\(A_1, A_2 , ..., A_n\)</span> in a way to minimizethe number of scalar multiplications. ***</p><p><span class="math display">\[A_1 = \left[\begin{matrix} 1 &amp; 2\end{matrix}\right] A_2 = \left[ \begin{matrix} 1 &amp; 2 &amp; 3 \\ 4&amp; 5 &amp; 6 \end{matrix} \right] A_3 =\left[\begin{matrix} 1 &amp; 2&amp; 3 &amp; 4\\ 4 &amp; 5 &amp; 6 &amp; 7\\7 &amp; 8 &amp; 9 &amp;10\end{matrix} \right]\]</span></p><p>Solutions:</p><p><span class="math inline">\(((A_1)(A_2))(A_3) : (1 \times 2 \times 3)+ (1 \times 3 \times 4)\)</span></p><p><span class="math inline">\((A_1)((A_2)(A_3)) : (2 \times 3 \times 4)+ (1 \times 2 \times 4)\)</span></p><p>我们要如何选取矩阵相乘的顺序来使乘的次数最小呢？</p><p>若枚举所有可能的解法，时间复杂度是指数级的。我们可以将其想象成一个多步决策，将加括号看成子问题。我们使用OPT(i,j)来表示子问题，原问题可以通过计算OPT(1,n)解决。由于<strong>最优子结构</strong>性质，我们可以通过计算子问题的最优解计算出原问题的最优解。</p><p><span class="math display">\[OPT(1,n) = OPT(1,k) + OPT(k+1,n) +p_1p_{k+1}p_{n+1}\]</span>我们可以通过<strong>枚举</strong>所有可能的解来获得第一步决策。</p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fwesfnsyvnj20e405a0t4.jpg"alt="recursive slt" /><figcaption aria-hidden="true">recursive slt</figcaption></figure><h2 id="trials">Trials</h2><h3 id="trial-1">Trial 1</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">RECURSIVE MATRIX CHAIN(i, j)</span><br><span class="line">    if i == j then </span><br><span class="line">        return 0; </span><br><span class="line">    end if </span><br><span class="line">    OPT(i, j) = +∞; </span><br><span class="line">    for k = i to j − 1 do </span><br><span class="line">        q = RECURSIVE MATRIX CHAIN(i, k) </span><br><span class="line">            + RECURSIVE MATRIX CHAIN(k + 1, j) + p_(i−1) p_k p_j ; </span><br><span class="line">        if q &lt; OPT(i, j) then </span><br><span class="line">            OPT(i, j) = q; </span><br><span class="line">        end if </span><br><span class="line">    end for </span><br><span class="line">    return OPT(i, j);</span><br></pre></td></tr></table></figure><p>最优解：RECURSIVE MATRIX CHAIN(1, n)</p><p>然而，这种方式需要花费指数时间<spanclass="math inline">\(O(n^2)\)</span>，且有大量的重复计算。我们可以开一个二维数组来存储subsolution，之后查表即可。</p><h3 id="trial-2">Trial 2</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MEMORIZE_MATRIX_CHAIN(i, j) </span><br><span class="line">    if OPT[i, j] != NULL then </span><br><span class="line">        return OPT(i, j); </span><br><span class="line">    end if </span><br><span class="line">    if i == j then </span><br><span class="line">        OPT[i, j] = 0; </span><br><span class="line">        else </span><br><span class="line">        for k = i to j − 1 do </span><br><span class="line">            q = MEMORIZE MATRIX CHAIN(i, k) </span><br><span class="line">                +MEMORIZE MATRIX CHAIN(k + 1, j) </span><br><span class="line">                    +p i−1 p k p j ; </span><br><span class="line">            if q &lt; OPT[i, j] then </span><br><span class="line">                OPT[i, j] = q; </span><br><span class="line">            end if </span><br><span class="line">        end for </span><br><span class="line">    end if </span><br><span class="line">    return OPT[i, j];</span><br></pre></td></tr></table></figure><p>原问题可以通过调用MEMORIZE_MATRIX_CHAIN(1, n)并将所有OPT[i,j]设为NULL来解决。时间复杂度为<spanclass="math inline">\(O(n^3)\)</span></p><h3 id="trial-3">Trial 3</h3><p>更快的一种解决方式：自底向上迭代 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">MATRIX CHAIN MULTIPLICATION(p 0 , p 1 , ..., p n )</span><br><span class="line">    for i = 1 to n do </span><br><span class="line">        OPT(i, i) = 0; </span><br><span class="line">    end for </span><br><span class="line">    for l = 2 to n do </span><br><span class="line">        for i = 1 to n − l + 1 do </span><br><span class="line">            j = i + l − 1; </span><br><span class="line">            OPT(i, j) = +∞; </span><br><span class="line">            for k = i to j − 1 do</span><br><span class="line">                q = OPT(i, k) + OPT(k + 1, j) + p i−1 p k p j ; </span><br><span class="line">                if q &lt; OPT(i, j) then </span><br><span class="line">                    OPT(i, j) = q; </span><br><span class="line">                    S(i, j) = k; </span><br><span class="line">                end if </span><br><span class="line">            end for </span><br><span class="line">        end for </span><br><span class="line">    end for </span><br><span class="line">    return OPT(1, n);</span><br></pre></td></tr></table></figure> <imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fwesfntaifj20nb089dgw.jpg" /></p><p>解题顺序：红 <span class="math inline">\(\Rightarrow\)</span> 绿<span class="math inline">\(\Rightarrow\)</span> 橙 <spanclass="math inline">\(\Rightarrow\)</span>蓝，自底而上，先列出所有子问题</p><p><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fwesfnswo9j20f207rdge.jpg" /></p><p><strong>step 1：</strong></p><p>OPT[1,2],OPT[2,3],OPT[3,4]</p><p><strong>step 2：</strong> <span class="math display">\[OPT[1,3] = min\left\{ \begin{aligned} OPT[1,2] + OPT[3,3] + p_0 \times p_2 \times p_3\\ OPT[1,1] + OPT[2,3] + p_0 \times p_1 \times p_3  \end{aligned}\right.\]</span> Thus, SPLITTER[1,2] = 2.</p><p><span class="math display">\[OPT[1,3] = min \left\{ \begin{aligned}OPT[2,2] + OPT[3,4] + p_1 \times p_2 \times p_4 \\ OPT[2,3] + OPT[4,4] +p_1 \times p_3 \times p_4  \end{aligned} \right.\]</span> Thus,SPLITTER[2,4] = 3.</p><p><strong>step 3：</strong> <span class="math display">\[OPT[1,3] = min\left\{ \begin{aligned} OPT[1,1] + OPT[2,4] + p_0 \times p_1 \times p_4\\ OPT[1,2] + OPT[3,4] + p_0 \times p_2 \times p_4 \\ OPT[1,3] +OPT[4,4] + p_0 \times p_3 \times p_4  \end{aligned} \right.\]</span>Thus, SPLITTER[1,4] = 3.</p><p>在此方法中，只有子问题的最优解被计算，因此没有经过遍历，不是指数级的复杂度。</p><h2 id="backtracking">Backtracking</h2><p>从OPT[1,n]开始回溯每一个决策阶段。</p><p><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fwesfnst9hj20eo07fmxk.jpg" /></p><p>Step 1: <span class="math inline">\((A_1A_2A_3)(A_4)\)</span></p><p>Step 2: <span class="math inline">\(((A_1A_2)(A_3))(A_4)\)</span></p><p>Step 3: <spanclass="math inline">\((((A_1)(A_2)(A_3))(A_4)\)</span></p><h1 id="knapsack-problem-01背包问题">0/1 KNAPSACK problem0/1背包问题</h1><hr /><p>INPUT: A set of items <span class="math inline">\(S = \{1, 2, ...,n\}\)</span>. Item i has weight <span class="math inline">\(w_i\)</span>and value <span class="math inline">\(v_i\)</span> . A total weightlimit W; OUTPUT: A subset of items to maximize the total value withtotal weight below W. ***这里的0和1表示我们可以选择一个物品(1)或者放弃它(0)，我们不能选择物品的一部分。</p><p>我们可以将将最优解表示为：<spanclass="math inline">\(OPT({1,2,...,i},w)\)</span></p><p>最优子结构： <span class="math display">\[OPT({1,2,...,n},W) = max\left\{\begin{array}{lr}OPT({1,2,...,n-1},W) \\OPT({1,2,...,n-1},W-w_n)+ v_n\end{array}\right.\]</span> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Knapsack(n, W)</span><br><span class="line">    for w = 1 to W do </span><br><span class="line">        OPT[0, w] = 0; </span><br><span class="line">    end for </span><br><span class="line">    for i = 1 to n do </span><br><span class="line">        for w = 1 to W do </span><br><span class="line">            OPT[i, w] = max&#123;OPT[i−1, w], vi +OPT[i−1, w−wi]&#125;; </span><br><span class="line">        end for </span><br><span class="line">    end for</span><br></pre></td></tr></table></figure>我们使用OPT[I,w]来表示OPT({1,2,...,i},w).</p><p><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fwesfnsy67j20bq05h74v.jpg" /></p><p><strong>step 1：</strong></p><p>Initially all OPT[0, w] = 0.</p><p><strong>step 2：</strong></p><p><span class="math display">\[OPT[1, 2] = max\{ OPT[0, 2], OPT[0, 0] +V_1 \} =2\]</span></p><p><strong>step 3：</strong></p><p><span class="math display">\[OPT[2, 4] = max\{ OPT[1, 4], OPT[1, 2] +V_2\} =4\]</span></p><p><strong>step 4：</strong></p><p><span class="math display">\[OPT[3, 3] = max\{OPT[2, 3], OPT[2, 0] +V_3\} =3\]</span></p><p><strong>backtracking</strong></p><p><span class="math display">\[OPT[3, 6] = max{ OPT[2, 6](= 4), OPT[2,3] + V_3 (= 2 + 3)} =5\]</span></p><p>Decision: Select item 3</p><p><span class="math display">\[OPT[2, 3] = max{ OPT[1, 3](= 2), OPT[1,1] + V_2 (= 0 + 2)} =2\]</span></p><p>Decision: Select item 2</p><p>时间复杂度：<span class="math inline">\(O(nW)\)</span></p><p>若将items看做是集合（recursion oversets），将花费指数倍的时间；但是若将items先进行排序（recursion oversequences），花费的时间为<spanclass="math inline">\(O(nW)\)</span>。</p><h1 id="sequence-alignment-problem">Sequence Alignment problem</h1><p><strong>Alignment</strong> is introduced to describe the generatingprocess of an erroneous word from the correct word using a series ofINS/DEL/MUTATION operations.</p><p>要比较两个序列，我们首先在合适的地方引入空格-。使得两个序列的长度相同，即<spanclass="math inline">\(|S&#39; = T&#39;|\)</span></p><p><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fwesfnrx4nj206s02rglr.jpg" /></p><p>对齐的字符可能有三种情况： 1. S'[i] = "-" : S'[i] is simply aDELETION of T'[i]. 2. T'[i] = "-" : S'[i] is simply an INSERTION. 3.Otherwise, S'[i] is a copy of T'[i] (with possible MUTATION).</p><p>我们可以用以下方式来给s(a,b)打分： 1. Match: +1 , e.g. s('C' ,'C' ) =1. 2. Mismatch: -1, e.g. s('E' , 'A' ) = −1. 3. Insertion/Deletion: -3,e.g. s('C' , '-' ) = −3.</p><p>使用对齐，我们可以找到最相似的来源，并且同样两个序列，通过不同的插入空格的方式我们可以得到不同的值。</p><hr /><p>INPUT:Two sequences S and T, |S| = m, and |T| = n; OUTPUT:To identifyan alignment of S and T that maximizes a pre-deﬁned scoring function.*** 这里也有三种需要考虑的情况： 1. <spanclass="math inline">\(S_m\)</span> comes from <spanclass="math inline">\(T_n\)</span>:S从T产生 2. <spanclass="math inline">\(S_m\)</span> is anINSERTION：若S的最后一个字母多敲了 3. <spanclass="math inline">\(S_m\)</span> comes from T[1..n − 1]：若S少敲了</p><p><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fwesfnutixj20lg05hdgb.jpg" /></p><p>note:蓝色框表示子问题</p><p>因此，我们可以将子问题的一般式设计为S的前缀(S[1..i])和T的的前缀(T[1..j])。最优解可以被表示为OPT(i,j).<span class="math display">\[OPT(i,j) = max\left\{\begin{array}{lr}s(S_i,T_j) + OPT(i-1, j-1) \\s(&#39;-&#39;,T_j)+ OPT(i, j-1) \\s(S_i,&#39;-&#39;) + OPT(i-1,j)\end{array}\right.\]</span> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Needleman-Wunsch(S, T)</span><br><span class="line">    for i = 0 to m; do   \\简单的初始化，为了方便计算</span><br><span class="line">        OPT[i, 0] = −3 ∗ i; </span><br><span class="line">    end for </span><br><span class="line">    for j = 0 to n; do </span><br><span class="line">        OPT[0, j] = −3 ∗ j; </span><br><span class="line">    end for </span><br><span class="line">    for i = 1 to m do </span><br><span class="line">        for j = 1 to n do </span><br><span class="line">            \\从三个单元里各自取了一些分，取最大</span><br><span class="line">            OPT[i,j]=max&#123;OPT[i−1,j−1]+s(Si,Tj),OPT[i-1,j]−3,OPT[i,j−1]−3&#125;;  </span><br><span class="line">        end for </span><br><span class="line">    end for </span><br><span class="line">    return OPT[m, n] ;</span><br></pre></td></tr></table></figure></p><p><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fwesfnwzbzj20dt0dgabl.jpg" /></p><p>之后通过<strong>回溯</strong>找到最优对齐方式（开一个表格方便回溯）：</p><p><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fwesfnxbxaj20dm0dlgng.jpg" /></p><p>在实践中，有许多中对齐方式可以达到与最优比对相似的效果，被称作次优比对。次优比对可以分为两类：1)类似的次优比对，2)不同的次优比对。</p><p>为了找到类似的次优对齐，我们使用采样技术追溯动态编程矩阵，即，不是在每个步骤采用最高得分选项，而是基于三个选项的值进行概率选择。e.g.将4以一定的概率回到之前的三个点，找一百回，用聚类的算法找到聚类中心。（最robust）</p><h2 id="优化">优化</h2><p>若输入的数据很大（e.g.输入了两篇文章）这个算法面临了三个问题： 1.占用空间太大 2. 用时太长 3. Local</p><h3 id="space-eﬃcient-algorithm">Space eﬃcient algorithm:</h3><p>Reducing the space requirement from O(mn) to O(m + n) (D. S.Hirschberg, 1975).</p><p>若只计算分数而不记录对齐信息，占用的空间很小。因为我们仅需占用2个数组。绿色数组：已填，蓝色数组：待填，一旦把蓝色的数组填完，绿色的数组就没用了。白色的部分都不需要存。</p><p><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fwesg4xtq3j20d50data5.jpg" /></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Prefix Space Efficient Alignment(S, T, score)</span><br><span class="line">    for i = 0 to m do </span><br><span class="line">        score[i] = −3 ∗ i;   \\绿色数组</span><br><span class="line">    end for </span><br><span class="line">    for i = 1 to m do </span><br><span class="line">        newscore[0] = 0;      \\蓝色数组</span><br><span class="line">        for j = 1 to n do </span><br><span class="line">            newscore[j] = max&#123;score[j − 1] + s(S i , T j ), score[j] 3, newscore[j − 1] − 3&#125;; </span><br><span class="line">        end for </span><br><span class="line">        for j = 1 to n do </span><br><span class="line">            score[j] = newscore[j]; </span><br><span class="line">        end for </span><br><span class="line">    end for </span><br><span class="line">    return score[n];</span><br></pre></td></tr></table></figure><p>相应地，我们也可以使用S和T的后缀获得分数和对齐方式。</p><p><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fwesg4mo4ej20dc0dkjsx.jpg" /></p><p>但是这种方式无法回溯对齐信息。因此，一种更聪明的方式是我们可以将S分为两半(where<span class="math inline">\(S_{\frac{m}{2}}\)</span>is alignedto,denoted as q)，分别计算对齐信息。 <spanclass="math display">\[OPT(S,T) =OPT(S[1...\frac{m}{2}],T[1..q])+OPT(S[\frac{m}{2}+1..m],T[q+1..n])\]</span></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Linear Space Alignment( S, T )</span><br><span class="line">    Allocate two arrays f and b; each array has a size of m Prefix Space Efficient Alignment(S[1.. m ], T, f); </span><br><span class="line">    Suffix Space Efficient Alignment(S[ m + 1..m], T, b); </span><br><span class="line">    Let q = argmaxi (f[i] + b[i]); </span><br><span class="line">    Free arrays f and b; </span><br><span class="line">    Record aligned-position &lt; m/2 , q &gt; in an array A; </span><br><span class="line">    Linear Space Alignment(S[1.. m ], T[1..q]); </span><br><span class="line">    Linear Space Alignment(S[ m + 1..m], T[q + 1..n]); </span><br><span class="line">    return A;</span><br></pre></td></tr></table></figure><p><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fwesg4o38wj20jp0f1mz5.jpg" /><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fwesg4nrcej20jq0f5gnj.jpg" /></p><p>先把S分为恰好左一半（前缀，forward）和右一半（后缀，backward），拿'OCUR'与T的总体作比较.从图中可以看出，分数一定是由S前一半的上半部分和后一半的下半部分产生。</p><p>由此，所需的空间仅为<span class="math inline">\(O(m+n)\)</span>.</p><h3 id="time-complexity">Time complexity</h3><p>我们可以发现，足够相似的是对角线，因此我们可以只关心条带内。 BandedDP仅需花费<span class="math inline">\(O(\alphan)\)</span>的时间复杂度。它为线性的算法，比原来快得多，条带外的部分我们可以不用考虑。</p><h3 id="local">Local</h3><p>全局比对：识别两个完整序列之间的相似性。</p><p>局部对齐：我们通常希望找到相似的段（子序列）。局部对齐的目的是识别两个序列的相似区段。其他区域可以被视为独立的，因此形成“随机匹配”。</p><p>通过增加额外的概率来改变递归： <span class="math display">\[OPT(i,j)= max \left\{\begin{array}{lr}0 \\s(S_i,T_j) + OPT(i-1, j-1)\\s(&#39;-&#39;,T_j) + OPT(i, j-1) \\s(S_i,&#39;-&#39;) + OPT(i-1,j)\end{array}\right.\]</span>其中0对应一个新的对齐。它与旧的对齐无关。</p><p><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fwesg4n05xj20aa0b9q4v.jpg" /></p><p>与从右下角回溯不同，这里从最大的项开始回溯，13为相似的终点，右下角的7为相似+不同。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Main message: 1. 把子问题的求解想象成多步求解过程 2.
子问题的最优解可以组合成原问题的最优解 3.
Programming：tabular可以被用于避免子问题的重复计算&lt;/p&gt;</summary>
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
    <category term="Algorithm Design and Analysis" scheme="http://yoursite.com/tags/Algorithm-Design-and-Analysis/"/>
    
    <category term="Basic algorithm design technique" scheme="http://yoursite.com/tags/Basic-algorithm-design-technique/"/>
    
    <category term="Dynamic programming" scheme="http://yoursite.com/tags/Dynamic-programming/"/>
    
    <category term="Optimization" scheme="http://yoursite.com/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>Algorithm Design and Analysis - Divide-and-Conquer</title>
    <link href="http://yoursite.com/2018/09/24/Algorithm-Design-and-Analysis-Divide-and-Conquer/"/>
    <id>http://yoursite.com/2018/09/24/Algorithm-Design-and-Analysis-Divide-and-Conquer/</id>
    <published>2018-09-24T06:44:42.000Z</published>
    <updated>2023-03-24T09:16:59.533Z</updated>
    
    <content type="html"><![CDATA[<p>Main message:</p><ol type="1"><li>从最简单的case入手</li><li>看能否分，能否combine</li><li>不求最优，只要次优</li></ol><span id="more"></span><h1 id="一remarks">一、Remarks</h1><p>1.对于ClosestPair问题，若暴力破解法将花费多项式时间，分治法通常可被用于节省运行时间：<spanclass="math inline">\(O(n^2)\)</span> <spanclass="math inline">\(\implies\)</span> <spanclass="math inline">\(O(nlogn)\)</span>2.这个技巧若与随机技巧结合将非常有威力</p><p>在使用分治法之前，需要先检验输入（i.e.问题是否可以被分为结构相似，规模更小的子问题）以及输出（i.e.原问题的解是否能够用子问题的解组合而成）</p><h1 id="二sort-problem">二、Sort Problem</h1><p>将长度为n的数组排序</p><pre><code>INPUT: An array of n integers, say A[0..n-1];OUTPUT: The items of A in increasing order.</code></pre><p><strong><em>方法1.将其分为n-1长度的数列和一个元素</em></strong></p><p>第一种方法可以从最简单的case入手，从n=2，n=3，到逐步解决原问题</p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fvl1co5hjuj20cw0atgmq.jpg"alt="分离出一个元素" /><figcaption aria-hidden="true">分离出一个元素</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">InsertionSort( A, n )</span><br><span class="line">for j = 0 to n - 1 do</span><br><span class="line">key = A[j];</span><br><span class="line">i = j - 1;</span><br><span class="line">while i &gt;= 0 and A[i] &gt; key do</span><br><span class="line">A[i + 1] = A[i];</span><br><span class="line">i --;</span><br><span class="line">end while</span><br><span class="line">A[i + 1] = key;</span><br><span class="line">end for</span><br></pre></td></tr></table></figure><p>最差的情况：A数组里的元素为逆序</p><p>时间复杂度： <spanclass="math inline">\(T(n)=T(n-1)+O(n)=O(n^2)\)</span></p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fvl1cnv0q4j206e0a3ab1.jpg"alt="最差情况" /><figcaption aria-hidden="true">最差情况</figcaption></figure><p><strong><em>方法2.将其分为两个独立的子问题(MERGESORT)</em></strong></p><p>将数组<span class="math inline">\(A[0..n-1]\)</span>分为两个数组<spanclass="math inline">\(A[0..\frac{n}{2}-1]\)</span>以及<spanclass="math inline">\(A[\frac{n}{2}-1..n-1]\)</span></p><figure><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fvl1cnxlrkj20f30armzr.jpg"alt="分为子问题" /><figcaption aria-hidden="true">分为子问题</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">MergeSort(A; l; r) //从l到r之间排序</span><br><span class="line">// To sort part of the array A[l::r]</span><br><span class="line">if l &lt; r then</span><br><span class="line">m = (l + r)=2; //m denotes the middle point</span><br><span class="line">MergeSort(A; l; m );</span><br><span class="line">MergeSort(A; m + 1; r);</span><br><span class="line">Merge(A; l; m; r); //Combining the sorted arrays</span><br><span class="line">end if</span><br><span class="line">Merge (A; l; m; r) //将左端最小与右端最小比较</span><br><span class="line">//Merge A[l::m] (denoted as L) and A[m + 1::r](denoted as R).</span><br><span class="line">i = 0; j = 0;</span><br><span class="line">for k = l to r do</span><br><span class="line">if L[i] &lt; R[j] then</span><br><span class="line">A[k] = L[i];</span><br><span class="line">i + +;</span><br><span class="line">else</span><br><span class="line">A[k] = R[j];</span><br><span class="line">j + +;</span><br><span class="line">end if</span><br><span class="line">end for</span><br></pre></td></tr></table></figure><p>时间复杂度：O(n)</p><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fvl1cnuxq5j20ea02v74x.jpg"alt="mergesort" /><figcaption aria-hidden="true">mergesort</figcaption></figure><h1id="三迭代的时间复杂度的分析方法">三、迭代的时间复杂度的分析方法：</h1><ol type="1"><li>Unrolling the recurrence 硬展开</li><li>Guess and substitution 猜然后验证</li><li>Master theorem</li></ol><h3 id="technique-1-unrolling-the-recurrence">technique 1: Unrolling therecurrence</h3><p>We have <span class="math inline">\(T(n) = 2T(\frac{n}{2}) + O(n)&lt;= 2T(\frac{n}{2}) + cn\)</span> for a constant c. Let unrolling afew levels to find a pattern, and then sum over all levels.</p><figure><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fvl1cnvn1mj20c408dt9u.jpg"alt="Unrolling" /><figcaption aria-hidden="true">Unrolling</figcaption></figure><h3 id="technique-2-guess-and-substitution">technique 2: Guess andsubstitution</h3><p>Guess and substitution: guess a solution, substitute it into therecurrence relation, and justify that it works.</p><figure><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fvl1cnxy0gj20gv08kq4g.jpg"alt="Guess" /><figcaption aria-hidden="true">Guess</figcaption></figure><h3 id="technique-3master-theorem-主定理">technique 3:Master theorem主定理</h3><p>Let T(n) be defined by <spanclass="math inline">\(T(n)=aT(\frac{n}{b})+O(n^d)\)</span> for a &gt; 1,b &gt; 1 and d &gt; 0, 其中n为问题规模，a为递推的子问题数量,<spanclass="math inline">\(\frac{n}{b}\)</span>为每个子问题的规模（假设每个子问题的规模基本一样），为递推以外进行的计算工作. then T(n) can be bounded by:</p><ol type="1"><li>If d &lt; <span class="math inline">\(\log_b a\)</span>, then <spanclass="math inline">\(T(n)=O(n^{\log_b a})\)</span>;</li><li>If d = <span class="math inline">\(\log_b a\)</span>, then <spanclass="math inline">\(T(n)=O(n^{\log_b a} \log n)\)</span>;</li><li>If d &gt; <span class="math inline">\(\log_b a\)</span>, then <spanclass="math inline">\(T(n)=O(n^d)\)</span>.</li></ol><h1 id="四counting-inversion-problem-数逆序对">四、Counting InversionProblem 数逆序对</h1><p>To count inversions in an array of n integers</p><pre><code>INPUT: An array $A[0..n]$ with n distinct numbers; OUTPUT:the number of inversions. A pair of indices i and j constitutes an inversion if i &lt; j butA[i] &gt; A[j].</code></pre><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fvl1cnupelj20ak041aa8.jpg"alt="CountingInversion" /><figcaption aria-hidden="true">CountingInversion</figcaption></figure><p>可使用分治法解决数逆序对问题，其中Divide和Conquer部分与之前排序问题相同，即将问题分为两个独立的子问题。Combine部分有两种策略。</p><p><strong>策略1</strong>：若<spanclass="math inline">\(A[0..\frac{n}{2}-1]\)</span>以及<spanclass="math inline">\(A[\frac{n}{2}..n-1]\)</span>没有特殊的结构，则我们需要检查所有可能的配对<spanclass="math inline">\((i,j)\)</span>去检查逆序。即若左右数都是任意的，就必须写For循环。</p><p>时间复杂度为：<spanclass="math inline">\(T(n)=2T(\frac{n}{2})+\frac{n^2}{4}=O(n^2)\)</span></p><figure><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fvl1cnyqh7j20d007h0uj.jpg"alt="strategy1" /><figcaption aria-hidden="true">strategy1</figcaption></figure><p><strong>策略2</strong>：若每一部分为升序，则相对容易计算逆序</p><figure><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fvl1co2bztj20cv06bjsg.jpg"alt="strategy2" /><figcaption aria-hidden="true">strategy2</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Sort-and-Count(A)</span><br><span class="line">Divide A into two sub-sequences L and R;</span><br><span class="line">(RCL, L) = Sort-and-Count(L);</span><br><span class="line">(RCR, R) = Sort-and-Count(R);</span><br><span class="line">(C, A) = Merge-and-Count(L, R);</span><br><span class="line">return (RC = RCL + RCR + C, A);</span><br><span class="line">Merge-and-Count (L; R)</span><br><span class="line">RC = 0; i = 0; j = 0;</span><br><span class="line">for k = 0 to ∥L∥ + ∥R∥ - 1 do</span><br><span class="line">if L[i] &gt; R[j] then</span><br><span class="line">A[k] = R[j];</span><br><span class="line">j + +;</span><br><span class="line">RC+ = (n/2 - i);</span><br><span class="line">else</span><br><span class="line">A[k] = L[i];</span><br><span class="line">i + +;</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">return (RC, A);</span><br></pre></td></tr></table></figure><p>时间复杂度为：<spanclass="math inline">\(T(n)=2T(\frac{n}{2})+O(n)=O(n\log n)\)</span></p><h1 id="五the-general-divide-and-conquer-paradigm">五、The generalDivide and Conquer paradigm</h1><h2 id="basic-idea">Basic Idea</h2><p>Many problems are recursive in structure, i.e., to solve a givenproblem, they call themselves several times to deal with closely relatedsub-problems. These sub-problems have the same form to the originalproblem but a smaller size.</p><h2 id="three-steps">Three Steps</h2><ol type="1"><li><strong><em>Divide</em></strong> a problem into a number ofindependent <strong><em>sub-problems</em></strong>;</li><li><strong><em>Conquer</em></strong> the subproblems by solving themrecursively;</li><li><strong><em>Combine</em></strong> the solutions to the subproblemsinto the solution to the original problem.</li></ol><h2 id="quick-sort-algorithm">Quick Sort algorithm</h2><p>Divide according to a randomly-selectedpivot.根据随机选取的轴进行分割。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">QuickSort(A)</span><br><span class="line">S_ = &#123;&#125;; S+ = &#123;&#125;;</span><br><span class="line">Choose a pivot A[j] uniformly at random;</span><br><span class="line">for i = 0 to n - 1 do  //将比A[j]小的放在左边，大的放在右边</span><br><span class="line">Put A[i] in S_ if A[i] &lt; A[j];</span><br><span class="line">Put A[i] in S+ if A[i] &gt;= A[j];</span><br><span class="line">end for</span><br><span class="line">QuickSort(S+);</span><br><span class="line">QuickSort(S_);</span><br><span class="line">Output S_, then A[j], then S+;</span><br></pre></td></tr></table></figure><ul><li>随机的操作使得这个算法变得简单而高效</li><li>然而，随机增加了分析的难度，我们不能保证取到每个问题的第<spanclass="math inline">\(\frac{n}{2}\)</span>个元素</li></ul><p><strong>最差的情况</strong>：选取到了每个迭代中最大/最小的元素</p><p><span class="math display">\[T(n) = T(n - 1) + O(n) =O(n^2)\]</span></p><p><strong>最好的情况</strong>：选取到了每个迭代中最中间的元素</p><p><span class="math display">\[T(n) = 2T(\frac{n}{2}) + O(n) = O(n\logn)\]</span></p><p><strong>大多数情况</strong>：选取到了一个接近中心的轴。这种情况我们认为期望的运行时间仍为：</p><p><span class="math display">\[T(n) = O(n\log n)\]</span></p><h2 id="analysis">Analysis</h2><p><strong>Observation 1:</strong> 对于任意i,j而言，<spanclass="math inline">\(A[i]\)</span>和<spanclass="math inline">\(A[j]\)</span>之间最多被比较一次。</p><p><strong>Observation 2:</strong> 当处理包含<spanclass="math inline">\(A[i..j]\)</span>的元素时，当且仅当<spanclass="math inline">\(A[i]\)</span>或<spanclass="math inline">\(A[j]\)</span>被选为枢轴时比较<spanclass="math inline">\(A[i]\)</span>和<spanclass="math inline">\(A[j]\)</span>。</p><h2 id="modified-quick-sort">Modified Quick Sort</h2><p>在原算法上寻找一个好的分点（大于<spanclass="math inline">\(/frac{n}{4}\)</span>小于<spanclass="math inline">\(/frac{3n}{4}\)</span>的点）。它在所有项目都不同时有用，然而耗内存，且在比原算法更慢，因为当它偏离中心点时不运行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ModifiedQuickSort(A)</span><br><span class="line">while TRUE do</span><br><span class="line">Choose a pivot A[j] uniformly at random;</span><br><span class="line">S_ = &#123;&#125;; S+ = &#123;&#125;;</span><br><span class="line">for i = 0 to n - 1 do</span><br><span class="line">Put A[i] in S_ if A[i] &lt; A[j];</span><br><span class="line">Put A[i] in S+ if A[i] &gt; A[j];</span><br><span class="line">end for</span><br><span class="line">if ∥S+∥ &gt;= n/4 and ∥S_∥ &gt;= n/4 then</span><br><span class="line">break;</span><br><span class="line">end if</span><br><span class="line">end while</span><br><span class="line">ModifiedQuickSort(S+);</span><br><span class="line">ModifiedQuickSort(S_);</span><br><span class="line">Output S_, then A[j], and finally S+;</span><br></pre></td></tr></table></figure><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fvnb5mr9sfj20bt04ojrv.jpg"alt="nearcenter" /><figcaption aria-hidden="true">nearcenter</figcaption></figure><p>比起中心轴，近中心轴更容易获得。获得一个中心点为轴的概率是<spanclass="math inline">\(frac{1}{n}\)</span>,而获得一个近中心轴的概率是<spanclass="math inline">\(frac{1}{2}\)</span>。因此，找到一个近中心轴的期望时间为2n。此时我们不追求最优的pivot，只选择足够好。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">\\Lomuto’s implementation</span><br><span class="line">\\in-place sort 不花内存。</span><br><span class="line">QuickSort(A; l; h)</span><br><span class="line">if l &lt; h then</span><br><span class="line">p =Partition(A; l; h);</span><br><span class="line">QuickSort(A; l; p - 1);</span><br><span class="line">QuickSort(A; p + 1; h);</span><br><span class="line">end if</span><br><span class="line">Partition(A; l; h)</span><br><span class="line">pivot = A[h]; i = l - 1;</span><br><span class="line">for j = l to h - 1 do</span><br><span class="line">if A[j] &lt; pivot then</span><br><span class="line">i + +;</span><br><span class="line">Swap A[i] with A[j];</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">if A[h] &lt; A[i + 1] then</span><br><span class="line">Swap A[i + 1] with A[h];</span><br><span class="line">end if</span><br><span class="line">return i + 1;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">\\Hoare’s implementation</span><br><span class="line">QuickSort(A; l; h)</span><br><span class="line">if l &lt; h then</span><br><span class="line">p =Partition(A; l; h);</span><br><span class="line">QuickSort(A; l; p); //Reason: A[p] might not be at its correct position</span><br><span class="line">QuickSort(A; p + 1; h);</span><br><span class="line">end if</span><br><span class="line">Partition(A; l; h)</span><br><span class="line">i = l - 1; j = h + 1; pivot = A[l];</span><br><span class="line">while TRUE do</span><br><span class="line">repeat</span><br><span class="line">j = j - 1;</span><br><span class="line">until A[j] &lt;= pivot or j == l;</span><br><span class="line">repeat</span><br><span class="line">i = i + 1;</span><br><span class="line">until A[i] &gt;= pivot or i == h;</span><br><span class="line">if i &gt;= j then</span><br><span class="line">return j;</span><br><span class="line">end if</span><br><span class="line">Swap A[i] with A[j];</span><br><span class="line">end while</span><br></pre></td></tr></table></figure><blockquote><blockquote><p>快速排序只在无重复元素时性能好，若有重复的元素，可用PARTITION算法来解决这个问题，即将数列分为三部分：比pivot大，与pivot相等，比pivot小。仅需对不等于pivot的partitions进行迭代排序。</p></blockquote></blockquote><h2 id="selection-problem">Selection problem</h2><p>找到数组中第K小的item</p><pre><code>INPUT:An array A = [A0, A1,.., An_1], and a number k &lt; n;OUTPUT:The k-th smallest item in general case (or the median of A as a specical case).</code></pre><p>若先将A排序再寻找第k个值，时间复杂度为<spanclass="math inline">\(O(nlogn)\)</span>。相反地，若使用分治法，则有可能开发出更快的算法（e.g.deterministiclinear algorithm by Blum et al.）。时间复杂度为<spanclass="math inline">\(O(n)\)</span>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Select(A; k)</span><br><span class="line">Choose an element Ai from A as a pivot;</span><br><span class="line">S+ = &#123;&#125;;</span><br><span class="line">S_ = &#123;&#125;;</span><br><span class="line">for j = 1 to n do</span><br><span class="line">if Aj &gt; Ai then</span><br><span class="line">S+ = S+ U &#123;Aj&#125;;</span><br><span class="line">else</span><br><span class="line">S_ = S_ U &#123;Aj&#125;;</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">if |S_| = k - 1 then</span><br><span class="line">return Ai;</span><br><span class="line">else if |S_| &gt; k - 1 then</span><br><span class="line">return Select(S_; k);  \\S的size越小越好</span><br><span class="line">else</span><br><span class="line">return Select(S+; k - |S_| + 1);</span><br><span class="line">end if</span><br></pre></td></tr></table></figure><h3 id="how-to-choose-a-pivot">How to choose a pivot?</h3><p>对于pivot（中心元），我们有三种选择方式：</p><p><strong>最差的选择</strong>：选取到了每个迭代中最小的元素(线性下降)</p><p><span class="math display">\[T(n) = T(n - 1) + O(n) =O(n^2)\]</span></p><p><strong>最好的选择</strong>：选取到了每个迭代中最中间的元素（指数级下降）</p><p><span class="math display">\[T(n) = T(\frac{n}{2}) + O(n) =O(n)\]</span></p><p><strong>好的选择</strong>：选取到了一个接近中心的pivot。（子问题最坏的情况也是指数级下降）</p><p><span class="math display">\[T(n) ≤ T((1 − ϵ)n) + O(n)≤ cn + c(1 − ϵ)n + c(1 − ϵ)^2 + ....=  O(n)\]</span></p><h3 id="how-to-select-a-nearly-central-pivot">How to select anearly-central pivot?</h3><p>我们可以通过以下几种方式尝试获得一个完整集合的中间值：</p><ul><li>Selecting a central pivot via <strong>examining medians ofgroups</strong></li><li>Selecting a central pivot via <strong>randomly selecting anelement</strong></li><li>Selecting a central pivot via <strong>examining a randomsample</strong></li></ul><h4id="strategy-1bfprt-algorithm-uses-median-of-medians-as-pivot">Strategy1:BFPRT algorithm uses median of medians as pivot</h4><figure><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fw46ro5745j20g803wgnc.jpg"alt="BFPRT" /><figcaption aria-hidden="true">BFPRT</figcaption></figure><p>SelectMedian(A) Line up elements in groups of 5 elements; 2. Find themedian of each group; //cost <spanclass="math inline">\(\frac{6n}{5}\)</span> time 3. Find the median ofmedians (denoted as M)through recursively running Select over the groupmedians; // <span class="math inline">\(T(\frac{n}{5})\)</span> time 4.Use M as pivot to partition A into S_ and S+; //<spanclass="math inline">\(O(n)\)</span> time 5. if |S_| = k - 1 then 6.return M; 7. else if |S_| &gt; k - 1 then 8. return Select(S_; k); //atmost <span class="math inline">\(T(\frac{7n}{10})\)</span> time 9. else10. return Select(S+; k - |S_| - 1); //at most <spanclass="math inline">\(T(\frac{7n}{10})\)</span> time 11. end if</p><p>这种方法较耗内存，我们也可以使用原位替换的算法：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">Select(A; l; r; k)</span><br><span class="line">while TRUE do</span><br><span class="line">if l == r then</span><br><span class="line">return l;</span><br><span class="line">end if</span><br><span class="line">p =Pivot(A; l; r); //Use median of medians A[p] as pivot ;</span><br><span class="line">pos =Partition(A; l; r; p); //pos represents the final position of the pivot, A[l..pos - 1] deposit S_ and A[pos + 1..r] deposit S+;</span><br><span class="line">if (k - 1) == pos then</span><br><span class="line">return k - 1;</span><br><span class="line">else if (k - 1) &lt; pos then</span><br><span class="line">r = pos - 1;</span><br><span class="line">else</span><br><span class="line">l = pos + 1;</span><br><span class="line">end if</span><br><span class="line">end while</span><br><span class="line">//</span><br><span class="line">Pivot(A, l, r)</span><br><span class="line">if (r - l) &lt; 5 then</span><br><span class="line">return Partition5(A, l, r); //Get median for 5 or less elements;</span><br><span class="line">end if</span><br><span class="line">for i = l to r by 5 do</span><br><span class="line">right = i + 4;</span><br><span class="line">if right &gt; r then</span><br><span class="line">right = r;</span><br><span class="line">end if</span><br><span class="line">m =Partition5(A, i, right); //Get median of a group;</span><br><span class="line">Swap A[m] and A[l + [(i-1)/5]];</span><br><span class="line">end for</span><br><span class="line">return Select(A, l, l + [(r-l)/5],(r-l)/10 + 1);</span><br><span class="line">//</span><br><span class="line">Partition(A, l, r, p)</span><br><span class="line">pivot = A[p];</span><br><span class="line">Swap A[p] and A[r]; //Move pivot to the right end;</span><br><span class="line">i = l;</span><br><span class="line">for j = l to r - 1 do</span><br><span class="line">if A[j] &lt; pivot then</span><br><span class="line">Swap A[i] and A[j];</span><br><span class="line">i + +;</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">Swap A[r] and A[i];</span><br><span class="line">return i;</span><br></pre></td></tr></table></figure><p>图解：</p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fw46ro5k3wj20i50cw422.jpg"alt="part1" /><figcaption aria-hidden="true">part1</figcaption></figure><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fw46ro5sauj20ga0czado.jpg"alt="part2" /><figcaption aria-hidden="true">part2</figcaption></figure><figure><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fw46ro5n6bj20gr0dc42f.jpg"alt="part3" /><figcaption aria-hidden="true">part3</figcaption></figure><h4id="strategy-2-quickselect-algorithm-randomly-select-an-element-as-pivot">Strategy2: QuickSelect algorithm randomly select an element as pivot</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">QuickSelect(A, k)</span><br><span class="line">Choose an element Ai from A uniformly at random;</span><br><span class="line">S+ = &#123;&#125;;</span><br><span class="line">S_ = &#123;&#125;;</span><br><span class="line">for all element Aj in A do</span><br><span class="line">if Aj &gt; Ai then</span><br><span class="line">S+ = S+ U &#123;Aj&#125;;</span><br><span class="line">else</span><br><span class="line">S_ = S_ U &#123;Aj&#125;;</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">if |S_| = k - 1 then</span><br><span class="line">return Ai;</span><br><span class="line">else if |S_| &gt; k - 1 then</span><br><span class="line">return QuickSelect(S_; k);</span><br><span class="line">else</span><br><span class="line">return QuickSelect(S+; k - |S_| - 1);</span><br><span class="line">end if</span><br></pre></td></tr></table></figure><p><strong>Basic idea:</strong> when selecting an element uniformly atrandom, it is highly likely to get a good pivot since a fairly largefraction of the elements are nearly-central.</p><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fw46ro56shj20fi07wwg5.jpg"alt="atRandom" /><figcaption aria-hidden="true">atRandom</figcaption></figure><p>The expected running time of QuickSelect: T(n) = O(n);</p><h4id="strategy-3-floyd-rivest-algorithm-selects-a-pivot-based-on-random-samples">Strategy3: Floyd-Rivest algorithm selects a pivot based on random samples</h4><figure><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fw46t80bu9j208605l0sw.jpg"alt="Floyd-Rivest" /><figcaption aria-hidden="true">Floyd-Rivest</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Floyd-Rivest-Select(A; k)</span><br><span class="line">Select a small random sample S (with replacement) from A.</span><br><span class="line">Select two pivots, denoted as u and v, from S through recursively calling Floyd-Rivest-Select. The interval [u, v], although small, is expected to cover the k-th smallest element of A.</span><br><span class="line">Divide A into three dis-joint subsets: L contains the elements less than u, M contains elements in [u; v], and H contains the elements greater than v.</span><br><span class="line">Partition A into these three sets through comparing each element Ai with u and v: if k &lt;= n2, Ai is compared with v first and then to u only if Ai &lt;= v. The order is reversed if k &gt; n2.</span><br><span class="line">The k-th smallest element of A is selected through recursively running over an appropriate subset.</span><br></pre></td></tr></table></figure><h1 id="六例题">六、例题</h1><p>note:一般来说，对于1维问题而言，我们可以将其分为两部分来解决问题。对于二维问题而言，我们可以将其分为横纵四个子问题来解决。但这并不是对所有情况都适用。</p><h2 id="closestpair-problem">1. ClosestPair problem</h2><p>Given a set of points in a plane, to find the closest pair.</p><pre><code>INPUT: n points in a plane;OUTPUT: The pair with the least Euclidean distance.</code></pre><p>暴力做法的时间复杂度为<spanclass="math display">\[T(n^2)\]</span>,存在很多的冗余。由于将平面分为四个部分会使点的分布不均，我们将平面分割为内含均匀数量的点的两个平面，分别计算两个平面内的最小对点距离，再计算跨越两个平面的最小距离。我们可以观察到，我们仅需检查中线周围以两个平面中较小的最小距离为宽度的区域就足够。</p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fw46ro22y6j20df07vgm1.jpg"alt="ClosestPair1" /><figcaption aria-hidden="true">ClosestPair1</figcaption></figure><p>并且，我们不需要检查条形区域内的所有点。检查它周围的11个点已足够。我们将两倍宽度的长条分解成格子，每个格子中最多包含一个点，否则表明之前计算出的最小距离有误。我们对y轴进行排序，然后仅需将每个点与其周围11个点进行比对。</p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fw46ro2bidj209b06qjrz.jpg"alt="ClosestPair2" /><figcaption aria-hidden="true">ClosestPair2</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ClosestPair(pl; :::; pr)</span><br><span class="line">//To find the closest points within (pl; :::; pr). Here we assume that pl,...,pr have already been sorted according to x-coordinate;</span><br><span class="line">if r - l == 1 then</span><br><span class="line">return d(pl; pr);</span><br><span class="line">end if</span><br><span class="line">Use the x-coordinate of p((l+r)/2) to divide pl,...,pr into two halves;</span><br><span class="line">d1 = ClosestPair(LeftHalf); //T(n2)</span><br><span class="line">d2 = ClosestPair(RightHalf); //T(n2)</span><br><span class="line">d = min(d1; d2);</span><br><span class="line">Sort points within the 2d wide strip by y-coordinate; //O(n log n)</span><br><span class="line">Scan points in y-order and calculate distance between each point with its next 11 neighbors. Update d if finding a distance less than d;  //O(n)</span><br></pre></td></tr></table></figure><p>Time-complexity: <span class="math display">\[T(n) = 2T(\frac{n}{2})+ O(n log n) = O(n log^2 n)\]</span>.</p><p>为了是复杂度降到O(n),我们可以引入structure，使用merge将每一个子问题先排序。</p><p><strong>实例：</strong></p><figure><imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fw46ro5d4aj20d909r40m.jpg"alt="ClosestPair3" /><figcaption aria-hidden="true">ClosestPair3</figcaption></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Main message:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;从最简单的case入手&lt;/li&gt;
&lt;li&gt;看能否分，能否combine&lt;/li&gt;
&lt;li&gt;不求最优，只要次优&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
    <category term="Algorithm Design and Analysis" scheme="http://yoursite.com/tags/Algorithm-Design-and-Analysis/"/>
    
    <category term="Basic algorithm design technique" scheme="http://yoursite.com/tags/Basic-algorithm-design-technique/"/>
    
    <category term="Divide-and-Conquer" scheme="http://yoursite.com/tags/Divide-and-Conquer/"/>
    
    <category term="Sort Algorithm" scheme="http://yoursite.com/tags/Sort-Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Algorithm Design and Analysis - Overview</title>
    <link href="http://yoursite.com/2018/09/14/Algorithm-Design-and-Analysis-Overview/"/>
    <id>http://yoursite.com/2018/09/14/Algorithm-Design-and-Analysis-Overview/</id>
    <published>2018-09-14T12:51:42.000Z</published>
    <updated>2023-03-24T09:17:09.634Z</updated>
    
    <content type="html"><![CDATA[<p>今天上了卜东波老师的第一次算法课，对算法的概况有了一个大体的了解，在这里对今天的内容做一些总结。【Threesolutions：Induction, Improvemrnt and Enumeration】</p><span id="more"></span><h2 id="一mind-map">一、Mind Map</h2><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fv9hdwxga9j20j60j0gmj.jpg"alt="MindMap" /><figcaption aria-hidden="true">MindMap</figcaption></figure><h2 id="二三种基本的算法策略">二、三种基本的算法策略</h2><p>基本上，所有的算法题都可以用三种策略解决。</p><ul><li><strong>Divide-and-conquer</strong>: Let's start from the "smallest"problem first, and investigate whether a large problem can<strong><em>reduce to smaller subproblems</em></strong>.</li></ul><p>分治法，将问题分解为小的问题，从最小问题出发解决问题。</p><p>See:<a  href ="http://imjiawen.com/2018/09/24/Algorithm-Design-and-Analysis-Divide-and-Conquer/">Algorithm_Design_andAnalysis-Divide-and-Conquer</a></p><ul><li><strong>"Intelligent" Enumeration</strong>: Consider an optimizationproblem. If the solution can be constructed step by step, we mightenumerate <strong><em>all possible complete solutions</em></strong> byconstructing a <strong><em>partial solution tree</em></strong>. Due tothe huge size of the search tree, some techniques should be employed toprune it.</li></ul><p>动态规划就是一种枚举所有可能性的算法，see：<a  href ="http://imjiawen.com/2018/09/24/Algorithm-Design-and-Analysis-Divide-and-Conquer/">AlgorithmDesign and Analysis - Dynamic programming</a></p><p>枚举法，枚举所有的解。可使用trick(如设定下界)来加快。</p><ul><li><strong>Improvement</strong>: Let's start from <strong><em>aninitial complete solution</em></strong>, and try to improve it step bystep.</li></ul><p>若无法将问题分解，就从质量不太好的完整解出发，逐步优化</p><h2 id="三例题">三、例题</h2><p><strong>EX1.Calculating the greatest common divisor (gcd)</strong>求最小公约数</p><p>The greatest common divisor of two integers a and b, when at leastone of them is not zero, is the largest positive integer that dividesthe numbers without a remainder.</p><pre><code>INPUT: two n-bits numbers a, and b (a &gt;= b)OUTPUT: gcd(a; b)</code></pre><p>已知gcd(1; 0) = 1;如何求解gcd(1949; 101)?</p><p>可将复杂问题分解成小的问题(辗转相除法)：gcd(1949; 101) = gcd(101;1949 mod 101) = gcd(101; 30)= gcd(30; 101 mod 30) = gcd(30; 11)= gcd(11;30 mod 11) = gcd(11; 8)= gcd(8; 3) = gcd(3; 2) = gcd(2; 1) = gcd(1; 0) =1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function Euclid(a; b)</span><br><span class="line">if b = 0 then</span><br><span class="line">return a;</span><br><span class="line">end if</span><br><span class="line">return Euclid(b; a mod b);</span><br></pre></td></tr></table></figure><p><strong>EX2.traveling salesman problem (TSP)</strong>周游城市的最短距离</p><pre><code>INPUT: n cities V = &#123;1; 2;...; n&#125;, and a distance matrix D, where dij (1 &lt;= i; j &lt;= n) denotes the distance between city i and j.OUTPUT: the shortest tour that visits each city exactly once and returns to the origin city.</code></pre><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fv9he4eu4dj204b04mmx9.jpg"alt="map1" /><figcaption aria-hidden="true">map1</figcaption></figure><p><strong><em>Trial 1: Divide and conquer</em></strong></p><p>我们可通过计算 M(S,e)来计算最小距离。S为要拜访的节点的集合，e为结束节点。</p><p>如，最短的距离我们可通过以下式子来计算：</p><p>min{ d2;1 +M({3; 4}; 2); d3;1 +M({2; 4}; 3); d4;1 +M({2; 3}; 4)}</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//算是一种动态规划算法</span><br><span class="line">function TSP(V;D)</span><br><span class="line">return min&#123; M(V - &#123;e&#125;; e) + de1&#125;;  //e属于V且e不为1</span><br><span class="line"></span><br><span class="line">function M(S; e)</span><br><span class="line">if S = &#123;v&#125; then</span><br><span class="line">M(S; e) = d1v + dve;</span><br><span class="line">return M(S; e);</span><br><span class="line">end if</span><br><span class="line">return min&#123; M(S - &#123;i&#125;; i) + dei&#125;;  //i属于S，i不为e</span><br></pre></td></tr></table></figure><p><strong><em>Trial 2: Improvement strategy</em></strong></p><p>从一个完整的解中开始，一步一步去完善它。Newton法，随机梯度下降和MC都用到了这个策略。但这种方法不一定能够获得最优解。可得到最优解的情况：线性规划，二次规划，网络流等。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function GenericImprovement(G;D)  //通用，逐步迭代完整解</span><br><span class="line">Let s be an initial tour;   //初始解的选择很重要</span><br><span class="line">while TRUE do</span><br><span class="line">Select a new tour s′ from the neighbourhood of s; //扰动越小越好</span><br><span class="line">if s′ is shorter than s then</span><br><span class="line">s = s′;</span><br><span class="line">end if</span><br><span class="line">if stopping(s) then  //s满足退出条件</span><br><span class="line">return s;</span><br><span class="line">end if</span><br><span class="line">end while</span><br></pre></td></tr></table></figure><p><strong><em>Trial 3: Intelligent" enumerationstrategy</em></strong></p><p>完整的解可以表示为n条边的排列。将边缘按照一定顺序排列，一个完整的解可以被表示为：X= [x1, x2,..., xm]。 例如：a -&gt; b -&gt; c -&gt; d -&gt; e -&gt; a可以被表示为 X = [1, 0, 0, 1, 1, 0, 0, 1, 0, 1]。所有的环游都可以写成这种形式，我们就能枚举出所有的解。</p><figure><imgsrc="https://wx3.sinaimg.cn/mw690/83fd5bdely1fv9he4hzsgj208a06ewf0.jpg"alt="map2" /><figcaption aria-hidden="true">map2</figcaption></figure><figure><imgsrc="https://wx4.sinaimg.cn/mw690/83fd5bdely1fv9heynqcqj20i2072gn3.jpg"alt="tree" /><figcaption aria-hidden="true">tree</figcaption></figure><p>子节点：表示一个完整的解</p><p>内部解：表示一个部分解，是一个已知item的子集</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function GenericBacktrack(P0)  //枚举所有的解</span><br><span class="line">Let A = fP0g. //Start with the original problem P0. Here, A</span><br><span class="line">denotes the active subproblems that are unexplored.</span><br><span class="line">best_so_far = 1;   //当前知道的最短路程</span><br><span class="line">while A ̸= NULL do  //当还有节点要扩展时</span><br><span class="line">Choose and remove a subproblem P from A;</span><br><span class="line">Expand P into smaller subproblems P1, P2,..., Pk;</span><br><span class="line">for i = 1 to k do</span><br><span class="line">if Pi corresponds to a complete solution then</span><br><span class="line">Update best_so_far if the corresponding objective function value is better;  //对应一个完整解</span><br><span class="line">else</span><br><span class="line">Insert Pi into A;   //对应一个部分解</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">end while</span><br><span class="line">return best_so_far;</span><br></pre></td></tr></table></figure><p>然而，这种方式计算量巨大，需要花费指数倍的时间，且我们无需在内存中存储整个树。因此，我们可以从中剔除一些低质量的或者不符合条件的部分解（贪心算法）。我们可以使用heuristicfunctions 和下界(lower boundfunctions)来评估部分解的质量。【这是EM算法的核心】</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function IntelligentBacktrack(P0)</span><br><span class="line">Let A = fP0g. // Start with the original problem P0. Here A denotes the active subproblems that are unexplored.</span><br><span class="line">best_so_far = infinite;</span><br><span class="line">while A ̸= NULL do</span><br><span class="line">Choose a subproblem P in A with lower bound less than best_so_far;</span><br><span class="line">Remove P from A;</span><br><span class="line">Expand P into smaller subproblems P1, P2, ..., Pk,</span><br><span class="line">for i = 1 to k do</span><br><span class="line">if Pi corresponds to a complete solution then</span><br><span class="line">Update best so far;</span><br><span class="line">else</span><br><span class="line">if lowerbound(Pi) &lt;= best so far then Insert Pi into A;</span><br><span class="line">end if</span><br><span class="line">end if</span><br><span class="line">end for</span><br><span class="line">end while</span><br><span class="line">return best_so_far;</span><br></pre></td></tr></table></figure><h2 id="四小结">四、小结</h2><ol type="1"><li>先观察问题的结构，解的形式，再设计算法</li><li>能分解成子问题是一个非常有效的信号</li><li>在优化问题中，下界信息非常重要</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天上了卜东波老师的第一次算法课，对算法的概况有了一个大体的了解，在这里对今天的内容做一些总结。【Three
solutions：Induction, Improvemrnt and Enumeration】&lt;/p&gt;</summary>
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
    <category term="Algorithm Design and Analysis" scheme="http://yoursite.com/tags/Algorithm-Design-and-Analysis/"/>
    
    <category term="Course summary" scheme="http://yoursite.com/tags/Course-summary/"/>
    
    <category term="Overview" scheme="http://yoursite.com/tags/Overview/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic programming 动态规划算法</title>
    <link href="http://yoursite.com/2018/08/14/Dynamic-programming-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/14/Dynamic-programming-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/</id>
    <published>2018-08-14T15:08:47.000Z</published>
    <updated>2023-03-24T09:17:13.272Z</updated>
    
    <content type="html"><![CDATA[<p>动态规划算法在编程题中运用场景很广(我觉得也有点难:()，这篇博文主要总结了动态规划算法的基本思想以及一些例题。</p><span id="more"></span><h2 id="一基本概念">一、基本概念</h2><p>动态规划过程是：每次决策依赖于当前状态，又随即引起状态的转移。一个决策序列就是在变化的状态中产生出来的，所以，这种<strong>多阶段最优化决策解决问题的过程</strong>就称为动态规划。</p><h2 id="二基本思想与策略">二、基本思想与策略</h2><p>动态规划算法与分治法的区别：<strong>适合于用动态规划法求解的问题，经分解后得到的子问题往往不是互相独立的（即下一个子阶段的求解是建立在上一个子阶段的解的基础上，进行进一步的求解）</strong>。</p><p>动态规划算法将待求解的问题分解为若干个子阶段，按顺序求解子阶段，前一子问题的解决为后一子问题的求解提供了有用信息。在求解任一子问题时，列出各种可能的局部解，通过决策保留那些有可能达到最优的局部解，丢弃其他局部解。依次解决各子问题，最后一个子问题就是初始问题的解。</p><p>由于动态规划解决的问题多数有重叠子问题这个特点，为减少重复计算，对每一个子问题只解一次，将其不同阶段的不同状态保存在一个二维数组中。</p><h2 id="三适用情况">三、适用情况</h2><p>一般动态规划问题具有三个性质：</p><ol type="1"><li>最优化原理：如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。</li><li>无后效性：即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关。</li><li>有重叠子问题：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质<strong>并不是动态规划适用的必要条件</strong>，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势）有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个<strong>表格</strong>中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率。</li></ol><p>note:先创建一个哈希表，将每次不同参数的计算结果存入哈希表，当遇到相同参数时，再从哈希表里取出，就避免了重复计算<strong>【备忘录算法】</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//上台阶问题中运用备忘录算法</span><br><span class="line">int getClimbingWays(int n, HashMap&lt;Integer, Integer&gt; map)&#123;</span><br><span class="line">    if(n&lt;1)&#123;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line">    if(n==1)&#123;</span><br><span class="line">        return 1;</span><br><span class="line">    &#125;</span><br><span class="line">     if(n==2)&#123;</span><br><span class="line">        return 2;</span><br><span class="line">    &#125;</span><br><span class="line">     if(map.contains(n))&#123;</span><br><span class="line">        return map.get(n);</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">        int value = getClimbingWays(n-1) + getClimbingWays(n-2);</span><br><span class="line">        map.put(n,value);</span><br><span class="line">        return value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四求解的基本步骤">四、求解的基本步骤</h2><p>动态规划所处理的问题是一个<strong>多阶段决策问题</strong>，一般由初始状态开始，通过对中间阶段决策的选择，达到结束状态。这些决策形成了一个决策序列，同时确定了完成整个过程的一条活动路线(通常是求最优的活动路线)。如下所示。动态规划的设计都有着一定的模式，一般要经历以下几个步骤。</p><p><strong>初始状态 -&gt; |决策1| -&gt; |决策2| -&gt; ... |决策n| -&gt;结束状态</strong></p><ol type="1"><li><strong>划分阶段</strong>：按照问题的时间或空间特征，把问题分为若干个阶段。在划分阶段时，注意<strong>划分后的阶段一定要是有序的或者是可排序的</strong>，否则问题就无法求解。</li><li><strong>确定状态和状态变量</strong>：将问题发展到各个阶段时所处于的各种客观情况用不同的状态表示出来。当然，状态的选择要满足<strong>无后效性</strong>。</li><li><strong>确定决策并写出状态转移方程</strong>：因为决策和状态转移有着天然的联系，<strong>状态转移就是根据上一阶段的状态和决策来导出本阶段的状态</strong>。所以如果确定了决策，状态转移方程也就可写出。但事实上常常是反过来做，<strong>根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程</strong>。</li><li><strong>寻找边界条件</strong>：给出的状态转移方程是一个递推式，需要一个递推的终止条件或边界条件。</li></ol><p>一般，只要解决问题的<strong>阶段</strong>、<strong>状态</strong>和<strong>状态转移决策</strong>确定了，就可以写出状态转移方程（包括边界条件）。实际应用中可以按以下几个简化的步骤进行设计：</p><ol type="1"><li>描述最优解的结构</li><li>递归定义最优解的值</li><li>按自底向上的方式计算最优解的值 //此三步构成动态规划解的基础</li><li>由计算出的结果构造一个最优解 //若只要求计算最优解的值可省略</li></ol><h2 id="五算法实现的说明">五、算法实现的说明</h2><p>动态规划的主要难点在于理论上的设计，也就是上面4个步骤的确定，一旦设计完成，实现部分就会非常简单。</p><pre><code> 使用动态规划求解问题，最重要的就是确定动态规划三要素：（1）问题的阶段 （2）每个阶段的状态（3）从前一个阶段转化到后一个阶段之间的递推关系。</code></pre><p>递推关系必须是从次小的问题开始到较大的问题之间的转化，从这个角度来说，动态规划往往可以用递归程序来实现，不过因为<strong>递推可以充分利用前面保存的子问题的解来减少重复计算，所以对于大规模问题来说，递归有不可比拟的优势</strong>，这也是动态规划算法的核心之处。</p><p>确定了动态规划的这三要素，<strong>整个求解过程就可以用一个最优决策表来描述，最优决策表是一个二维表，其中行表示决策的阶段，列表示问题状态</strong>，表格需要<strong>填写的数据一般对应此问题的在某个阶段某个状态下的最优值</strong>（如最短路径，最长公共子序列，最大价值等），填表的过程就是根据递推关系，从1行1列开始，以行或者列优先的顺序，依次填写表格，最后根据整个表格的数据通过简单的取舍或者运算求得问题的最优解。</p><p><em>f(n,m)=max{f(n-1,m), f(n-1,m-w[n])+P(n,m)}</em></p><h2 id="六经典动态算法题">六、经典动态算法题</h2><p><strong>例题一：上台阶问题</strong></p><p>有n级台阶，一个人每次上一级或者两级，问有多少种走完n级台阶的方法。</p><p>分析：动态规划的实现的关键在于能不能准确合理的用动态规划表来抽象出实际问题。在这个问题上，我们让f(n)表示走上n级台阶的方法数。</p><p>那么当n为1时，f(n) = 1,n为2时，f(n)=2,就是说当台阶只有一级的时候，方法数是一种，台阶有两级的时候，方法数为2。那么当我们要走上n级台阶，必然是从n-1级台阶迈一步或者是从n-2级台阶迈两步，所以到达n级台阶的方法数必然是到达n-1级台阶的方法数加上到达n-2级台阶的方法数之和。即f(n)=f(n-1)+f(n-2)，我们用dp[n]来表示动态规划表，dp[i],i&gt;0,i&lt;=n,表示到达i级台阶的方法数。</p><p><strong><em>迭代法（自顶向下）</em></strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/*dp是全局数组，大小为n,全部初始化为0，是题目中的动态规划表*/</span><br><span class="line">int fun(int n)&#123;</span><br><span class="line">if (n==1||n==2)</span><br><span class="line">return n;</span><br><span class="line">/*判断n-1的状态有没有被计算过*/</span><br><span class="line">if (!dp[n-1])</span><br><span class="line">dp[n-1] = fun(n-1);</span><br><span class="line">if(!dp[n-2])</span><br><span class="line">dp[n-2]=fun(n-2);</span><br><span class="line">return dp[n-1]+dp[n-2];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>LeetCode相关问题： <imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fv2hkwxy7kj20ez0c4q32.jpg"alt="leetcode问题" /> <imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fv2hkwpb75j20dv06kdfs.jpg"alt="解法" /></p><p><strong><em>动态规划求解（自底向上）</em></strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">int getClimbingWays(int n, HashMap&lt;Integer, Integer&gt; map)&#123;</span><br><span class="line">    if(n&lt;1)&#123;</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line">    if(n==1)&#123;</span><br><span class="line">        return 1;</span><br><span class="line">    &#125;</span><br><span class="line">    if(n==2)&#123;</span><br><span class="line">        return 2;</span><br><span class="line">    &#125;</span><br><span class="line">    int a = 1;</span><br><span class="line">    int b = 2;</span><br><span class="line">    int tmp = 0;</span><br><span class="line">    </span><br><span class="line">    for (int i = 3; i&lt;=n; i++)&#123;</span><br><span class="line">        tmp = a + b;</span><br><span class="line">        a = b;</span><br><span class="line">        b = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    return tmp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>例题二：国王与金矿（与01背包问题类似）</strong></p><p>有一个国家发现了5座金矿，每座金矿的黄金储量不同，需要参与挖掘的工人数也不同。参与挖矿工人的总数是10人。每座金矿要么全挖，要么不挖，不能派出一半人挖取一半金矿。要求用程序求解出，要想得到尽可能多的黄金，应该选择挖取哪几座金矿？</p><p>500金/5人；200金/3人；300金/4人；350金/3人；400金/5人</p><p>问题的状态转移方程式：</p><p>F(n,w) = 0 (n&lt;=1, w&lt;p[0]);//若给定的工人数量不够挖取第一座金矿</p><p>F(n,w) = g[0] (n==1, w&gt;=p[0]);</p><p>F(n,w) = F(n-1,w) (n&gt;1, w&lt;p[n-1])</p><p>F(n,w) = max(F(n-1,w), F(n-1,w-p[n-1])+g[n-1]) (n&gt;1, w&gt;=p[n-1])//最后一个矿有选择不挖和挖两个选择，在其中选最优的选择</p><p><strong><em>排列组合</em></strong></p><p>每一座金矿都有挖与不挖两种选择，如果有N座金矿，排列组合起来就有2<sup>N种选择。对所有可能性做遍历，排除那些使用工人数超过10的选择，在剩下的选择里找出获得金币数最多的选择。（O(2</sup>N)）</p><p><strong><em>简单递归</em></strong></p><p>把状态转移方程式翻译成递归程序，递归的结束的条件就是方程式当中的边界。因为每个状态有两个最优子结构，所以递归的执行流程类似于一颗高度为N的二叉树。方法的时间复杂度是O(2^N)。</p><p><strong><em>备忘录算法</em></strong></p><p>在简单递归的基础上增加一个HashMap备忘录，用来存储中间结果。HashMap的Key是一个包含金矿数N和工人数W的对象，Value是最优选择获得的黄金数。方法的时间复杂度和空间复杂度相同，都等同于备忘录中不同Key的数量。</p><p><strong><em>动态规划</em></strong></p><figure><imgsrc="https://wx2.sinaimg.cn/mw690/83fd5bdely1fv28bf1ocoj20hz04djti.jpg"alt="计算过程，除了第一行以外，每个格子都是由前一行的一个或者两个格子推导而来" /><figcaptionaria-hidden="true">计算过程，除了第一行以外，每个格子都是由前一行的一个或者两个格子推导而来</figcaption></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">int getMostGold(int n, int w, int[] g, int[] p)&#123;</span><br><span class="line">    int[] preResults = new int[p.length];</span><br><span class="line">    int[] results = new int[p.length];</span><br><span class="line">    //填充边界格子的值</span><br><span class="line">    for(int i=0; i&lt;=n; i++)&#123;</span><br><span class="line">        if(i &lt; p[0])&#123;</span><br><span class="line">            preResults[i] = 0;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            preResults[i] = g[0];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    //填充其余格子的值，外层循环是金矿数量，内层循环是工人数</span><br><span class="line">    for(int i=0; i&lt;=n; i++)&#123;</span><br><span class="line">        for(int j=0;j&lt;=w;j++)&#123;</span><br><span class="line">            if(j &lt; p[i])&#123;</span><br><span class="line">                results[j]=preResults[j]; //这里代码有问题，java中数组不能直接赋值，可改用clone方式赋值</span><br><span class="line">            &#125;else&#123;</span><br><span class="line">                results[j]=Math.max(preResults[j],preResults[j-p[i]]+g[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        preResults = results;</span><br><span class="line">    &#125;</span><br><span class="line">    return preResults[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>方法的时间复杂度是 O(n *w)，空间复杂度是(w)。需要注意的是，当金矿只有5座的时候，动态规划的性能优势还没有体现出来。当金矿有10座，甚至更多的时候，动态规划就明显具备了优势。由于动态规划方法的时间和空间都和W成正比，而简单递归却和W无关，所以当工人数量很多的时候，动态规划反而不如递归。</p><p><strong>例题三：最长公共子序列</strong></p><p>如果字符串一的所有字符按其在字符串中的顺序出现在另外一个字符串二中，则字符串一称之为字符串二的子串。</p><p>注意，并不要求子串（字符串一）的字符必须连续出现在字符串二中。请编写一个函数，输入两个字符串，求它们的最长公共子串，并打印出最长公共子串。例如：输入两个字符串BDCABA和ABCBDAB，字符串BCBA和BDAB都是是它们的最长公共子串，则输出它们的长度4，并打印任意一个子串。</p><p>LCS问题具有重叠子问题的性质，因此有些子问题可通过“查表”而直接得到。</p><p>用二维数组c[i][j]记录串x1x2⋯xi与y1y2⋯yj的LCS长度，则可得到状态转移方程<imgsrc="https://wx1.sinaimg.cn/mw690/83fd5bdely1fv2hb144upj20gf03kwfd.jpg"alt="状态转移方程" /></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public class LCSequence &#123;</span><br><span class="line">    //求解str1 和 str2 的最长公共子序列</span><br><span class="line">    public static int LCS(String str1, String str2)&#123;</span><br><span class="line">        int[][] c = new int[str1.length() + 1][str2.length() + 1];</span><br><span class="line">//初始化</span><br><span class="line">        for(int row = 0; row &lt;= str1.length(); row++)</span><br><span class="line">            c[row][0] = 0;</span><br><span class="line">        for(int column = 0; column &lt;= str2.length(); column++)</span><br><span class="line">            c[0][column] = 0;</span><br><span class="line">        </span><br><span class="line">        for(int i = 1; i &lt;= str1.length(); i++)</span><br><span class="line">            for(int j = 1; j &lt;= str2.length(); j++)</span><br><span class="line">            &#123;</span><br><span class="line">                if(str1.charAt(i-1) == str2.charAt(j-1))</span><br><span class="line">                    c[i][j] = c[i-1][j-1] + 1;</span><br><span class="line">                else if(c[i][j-1] &gt; c[i-1][j])</span><br><span class="line">                    c[i][j] = c[i][j-1];</span><br><span class="line">                else</span><br><span class="line">                    c[i][j] = c[i-1][j];</span><br><span class="line">            &#125;</span><br><span class="line">        return c[str1.length()][str2.length()];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="reference">Reference：</h2><ol type="1"><li>http://www.cnblogs.com/steven_oyj/archive/2010/05/22/1741374.html</li><li>http://www.cnblogs.com/pengyingh/articles/2396427.html</li><li>https://www.sohu.com/a/153858619_466939</li><li>https://www.cnblogs.com/hapjin/p/5572483.html</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;动态规划算法在编程题中运用场景很广(我觉得也有点难:(
)，这篇博文主要总结了动态规划算法的基本思想以及一些例题。&lt;/p&gt;</summary>
    
    
    
    <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
    <category term="Dynamic programming" scheme="http://yoursite.com/tags/Dynamic-programming/"/>
    
    <category term="Algorithm summary" scheme="http://yoursite.com/tags/Algorithm-summary/"/>
    
  </entry>
  
  <entry>
    <title>Hello world</title>
    <link href="http://yoursite.com/2018/05/01/Hello-world/"/>
    <id>http://yoursite.com/2018/05/01/Hello-world/</id>
    <published>2018-05-01T02:11:12.000Z</published>
    <updated>2020-06-05T02:11:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hello guys, this is Jiawen. Nice to meeting you :)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello guys, this is Jiawen. Nice to meeting you :)&lt;/p&gt;
</summary>
      
    
    
    
    <category term="life" scheme="http://yoursite.com/categories/life/"/>
    
    
    <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
    <category term="TheFirstPost" scheme="http://yoursite.com/tags/TheFirstPost/"/>
    
  </entry>
  
</feed>
